"""
XTTS-v2 TTS Engine wrapper
"""
import uuid
import asyncio
import threading
import warnings
from pathlib import Path
from typing import Optional, List, Dict
import re
import time
from TTS.api import TTS
import torch
import numpy as np
import backend.config as config
from num2words import num2words
from TTS.tts.layers.xtts import tokenizer as xtts_tokenizer

# Potlaƒçen√≠ deprecation warning z librosa (pkg_resources je zastaral√©, ale knihovna ho je≈°tƒõ pou≈æ√≠v√°)
warnings.filterwarnings("ignore", message=".*pkg_resources is deprecated.*", category=UserWarning)
from backend.config import (
    DEVICE,
    XTTS_MODEL_NAME,
    MODEL_CACHE_DIR,
    OUTPUTS_DIR,
    USE_SMALL_MODELS,
    ENABLE_CPU_OFFLOAD,
    FORCE_DEVICE,
    DEVICE_FORCED,
    ENABLE_AUDIO_ENHANCEMENT,
    AUDIO_ENHANCEMENT_PRESET,
    QUALITY_PRESETS,
    TARGET_SAMPLE_RATE,
    OUTPUT_SAMPLE_RATE,
    OUTPUT_HEADROOM_DB,
    ENABLE_MULTI_PASS,
    MULTI_PASS_COUNT,
    ENABLE_BATCH_PROCESSING,
    MAX_CHUNK_LENGTH,
    ENABLE_PROSODY_CONTROL,
    ENABLE_INTONATION_PROCESSING,
    ENABLE_PHONETIC_TRANSLATION,
    ENABLE_CZECH_TEXT_PROCESSING,
    ENABLE_DIALECT_CONVERSION,
    DIALECT_CODE,
    DIALECT_INTENSITY
)
from backend.audio_enhancer import AudioEnhancer
from backend.vocoder_hifigan import get_hifigan_vocoder
from backend.phonetic_translator import get_phonetic_translator

# Monkey patch pro spr√°vnou podporu ƒçe≈°tiny v num2words (TTS upstream pou≈æ√≠v√° k√≥d "cz")
try:
    def _expand_number_cs(m, lang="en"):
        lang_code = "cs" if lang.split("-")[0] == "cs" else lang
        return num2words(int(m.group(0)), lang=lang_code)

    def _expand_ordinal_cs(m, lang="en"):
        lang_code = "cs" if lang.split("-")[0] == "cs" else lang
        return num2words(int(m.group(1)), ordinal=True, lang=lang_code)

    xtts_tokenizer._expand_number = _expand_number_cs
    xtts_tokenizer._expand_ordinal = _expand_ordinal_cs
except Exception as patch_err:
    # Nechceme spadnout p≈ôi importu ‚Äì jen zalogujeme
    print(f"Warning: Czech number expansion patch not applied: {patch_err}")


class XTTSEngine:
    """Wrapper pro XTTS-v2 TTS engine"""

    def __init__(self):
        self.model: Optional[TTS] = None
        self.device = DEVICE
        self.is_loading = False
        self.is_loaded = False
        self.vocoder = get_hifigan_vocoder()
        # None = je≈°tƒõ nezkou≈°eno, False = nen√≠ dostupn√©, jinak tokenizer instance
        self._bpe_tokenizer = None

    def _get_bpe_tokenizer(self):
        """
        Vytvo≈ô√≠/vr√°t√≠ XTTS BPE tokenizer (stejn√Ω tokenizer.json jako upstream XTTS).
        Pou≈æ√≠v√° se pro poƒç√≠t√°n√≠ token≈Ø a bezpeƒçn√© dƒõlen√≠ textu pod limit 400 token≈Ø.
        """
        if self._bpe_tokenizer is False:
            return None
        if self._bpe_tokenizer is not None:
            return self._bpe_tokenizer

        def _silence_len_warnings(tok_obj):
            # VoiceBpeTokenizer.encode() vol√° check_input_length(), kter√° printuje warningy
            # p≈ôi p≈ôekroƒçen√≠ char limitu (typicky 186 pro cs). To je pro n√°s p≈ôi token-countingu
            # velmi hluƒçn√© a nen√≠ to chyba, tak≈æe to zti≈°√≠me.
            try:
                if hasattr(tok_obj, "check_input_length"):
                    tok_obj.check_input_length = lambda *_args, **_kwargs: None
            except Exception:
                pass

        # 1) Zkus tokenizer p≈ô√≠mo z modelu (nejspolehlivƒõj≈°√≠)
        try:
            if self.model is not None and hasattr(self.model, "synthesizer"):
                tts_model = getattr(self.model.synthesizer, "tts_model", None)
                model_tokenizer = getattr(tts_model, "tokenizer", None)
                if model_tokenizer is not None:
                    _silence_len_warnings(model_tokenizer)
                    self._bpe_tokenizer = model_tokenizer
                    return self._bpe_tokenizer
        except Exception:
            pass

        # 2) Fallback: tokenizer.json z bal√≠ƒçku (ne v≈°echny instalace ho bohu≈æel obsahuj√≠)
        try:
            candidate = Path(getattr(xtts_tokenizer, "DEFAULT_VOCAB_FILE", "")).resolve()
            if not candidate.exists():
                # V nƒõkter√Ωch build/instalac√≠ch je tokenizer.json ulo≈æen v assets (tortoise)
                base_tts_dir = Path(xtts_tokenizer.__file__).resolve().parents[2]  # .../TTS/tts
                alt = base_tts_dir / "utils" / "assets" / "tortoise" / "tokenizer.json"
                if alt.exists():
                    candidate = alt.resolve()

            if candidate.exists():
                tok = xtts_tokenizer.VoiceBpeTokenizer(str(candidate))
                _silence_len_warnings(tok)
                self._bpe_tokenizer = tok
                return self._bpe_tokenizer
        except Exception as e:
            print(f"Warning: XTTS tokenizer init failed: {e}")

        # 3) Nedostupn√© ‚Üí nech√°me None a nebudeme znovu zkou≈°et (bez spamov√°n√≠ warning≈Ø)
        self._bpe_tokenizer = False
        return None

    def _count_xtts_tokens(self, text: str, language: str = "cs") -> Optional[int]:
        """Vr√°t√≠ poƒçet XTTS token≈Ø pro dan√Ω text, nebo None pokud se to nepovede."""
        tok = self._get_bpe_tokenizer()
        if tok is None:
            return None
        try:
            # VoiceBpeTokenizer m√° encode(txt, lang) ‚Üí ids
            if hasattr(tok, "encode"):
                return len(tok.encode(text, language))
        except Exception:
            return None
        return None

    def _split_text_by_xtts_tokens(self, text: str, language: str = "cs") -> List[str]:
        """
        Rozsek√° text tak, aby ≈æ√°dn√Ω chunk nep≈ôekroƒçil config.XTTS_TARGET_MAX_TOKENS.
        Preferuje dƒõlen√≠ na konc√≠ch vƒõt, pak na slovech, a nakonec nouzovƒõ po znac√≠ch.
        """
        max_tokens = getattr(config, "XTTS_TARGET_MAX_TOKENS", 380)
        text = re.sub(r"\s+", " ", (text or "").strip())
        if not text:
            return []

        # Pokud tokenizer nen√≠ dostupn√Ω, dr≈æ se konzervativn√≠ho char splitu (bez overlap = ≈æ√°dn√© opakov√°n√≠)
        if self._get_bpe_tokenizer() is None:
            try:
                from backend.text_splitter import TextSplitter
                return TextSplitter.split_text(text, max_length=MAX_CHUNK_LENGTH, overlap=0)
            except Exception:
                # √∫pln√Ω fallback: hrub√© dƒõlen√≠ po MAX_CHUNK_LENGTH znac√≠ch
                return [text[i:i + MAX_CHUNK_LENGTH].strip() for i in range(0, len(text), MAX_CHUNK_LENGTH) if text[i:i + MAX_CHUNK_LENGTH].strip()]

        n = self._count_xtts_tokens(text, language)
        if n is not None and n <= max_tokens:
            return [text]

        def split_hard_by_chars(s: str) -> List[str]:
            out: List[str] = []
            s = s.strip()
            if not s:
                return out
            start = 0
            while start < len(s):
                # bin√°rn√≠ vyhled√°n√≠ nejdel≈°√≠ho prefixu, kter√Ω se vejde do token budgetu
                lo = start + 1
                hi = len(s)
                best = None
                while lo <= hi:
                    mid = (lo + hi) // 2
                    part = s[start:mid].strip()
                    if not part:
                        lo = mid + 1
                        continue
                    tn = self._count_xtts_tokens(part, language)
                    if tn is None:
                        # fallback: kdy≈æ sel≈æe tokenizer, ≈ôe≈æeme po MAX_CHUNK_LENGTH znac√≠ch
                        best = min(start + MAX_CHUNK_LENGTH, len(s))
                        break
                    if tn <= max_tokens:
                        best = mid
                        lo = mid + 1
                    else:
                        hi = mid - 1

                if best is None:
                    best = start + 1
                chunk = s[start:best].strip()
                if chunk:
                    out.append(chunk)
                start = best
            return out

        def split_by_words(sentence: str) -> List[str]:
            words = [w for w in sentence.strip().split(" ") if w]
            out: List[str] = []
            cur = ""
            for w in words:
                cand = w if not cur else f"{cur} {w}"
                tn = self._count_xtts_tokens(cand, language)
                if tn is not None and tn <= max_tokens:
                    cur = cand
                    continue

                if cur:
                    out.append(cur)
                    cur = w
                    # Pokud i samotn√© slovo/fragment p≈ôet√©k√°, ≈ôe≈æ tvrdƒõ
                    if (self._count_xtts_tokens(cur, language) or (max_tokens + 1)) > max_tokens:
                        out.extend(split_hard_by_chars(cur))
                        cur = ""
                else:
                    out.extend(split_hard_by_chars(w))
                    cur = ""

            if cur:
                out.append(cur)
            return out

        # Prim√°rnƒõ dƒõlen√≠ na vƒõty
        sentences = re.split(r"(?<=[.!?‚Ä¶])\s+", text)
        chunks: List[str] = []
        cur = ""
        for s in sentences:
            s = (s or "").strip()
            if not s:
                continue
            cand = s if not cur else f"{cur} {s}"
            tn = self._count_xtts_tokens(cand, language)
            if tn is not None and tn <= max_tokens:
                cur = cand
                continue

            if cur:
                chunks.append(cur)
                cur = ""

            # samotn√° vƒõta je dlouh√° ‚Üí rozdƒõlit podle slov / nouzovƒõ po znac√≠ch
            if (self._count_xtts_tokens(s, language) or (max_tokens + 1)) <= max_tokens:
                cur = s
            else:
                chunks.extend(split_by_words(s))

        if cur:
            chunks.append(cur)

        # Posledn√≠ pojistka: kdyby cokoli p≈ôeteklo (nap≈ô. tokenizer None), do≈ôe≈æ
        safe_chunks: List[str] = []
        for ch in chunks:
            tn = self._count_xtts_tokens(ch, language)
            if tn is None or tn <= max_tokens:
                safe_chunks.append(ch)
            else:
                safe_chunks.extend(split_hard_by_chars(ch))

        return [c for c in safe_chunks if c.strip()]

    async def load_model(self):
        """Naƒçte XTTS-v2 model asynchronnƒõ"""
        if self.is_loaded:
            return

        if self.is_loading:
            # Poƒçkej a≈æ se model naƒçte
            while self.is_loading:
                await asyncio.sleep(0.5)
            return

        self.is_loading = True

        try:
            print(f"Loading XTTS-v2 on {self.device}...")

            # Naƒçten√≠ modelu v thread poolu (TTS nen√≠ async)
            loop = asyncio.get_event_loop()
            self.model = await loop.run_in_executor(
                None,
                self._load_model_sync
            )

            self.is_loaded = True
            print("Model loaded successfully!")

        except Exception as e:
            print(f"Error loading model: {str(e)}")
            raise
        finally:
            self.is_loading = False

    def _load_model_sync(self) -> TTS:
        """Synchronn√≠ naƒçten√≠ modelu z Hugging Face nebo lok√°ln√≠ cache"""
        print(f"Loading model: {XTTS_MODEL_NAME}")
        print("Model bude sta≈æen z Hugging Face, pokud nen√≠ v cache...")

        try:
            # Zkus nejprve TTS registry n√°zev (stabilnƒõj≈°√≠)
            if XTTS_MODEL_NAME.startswith("coqui/"):
                # P≈ôevod z Hugging Face form√°tu na TTS registry
                model_name = "tts_models/multilingual/multi-dataset/xtts_v2"
                print(f"Trying TTS registry name: {model_name}")
            else:
                model_name = XTTS_MODEL_NAME

            # Naƒçten√≠ modelu s explicitn√≠m nastaven√≠m
            # Pou≈æijeme GPU pouze pokud je device nastaven na "cuda"
            use_gpu = (self.device == "cuda" and torch.cuda.is_available())
            model = TTS(
                model_name=model_name,
                progress_bar=True
            )

            # Optimalizace pro GPU s omezenou VRAM (6GB)
            if use_gpu and (USE_SMALL_MODELS or ENABLE_CPU_OFFLOAD):
                print("Applying GPU memory optimizations for 6GB VRAM...")
                if hasattr(model, 'synthesizer') and hasattr(model.synthesizer, 'tts_model'):
                    # Offload ƒç√°sti modelu na CPU pokud je pot≈ôeba
                    if ENABLE_CPU_OFFLOAD:
                        print("CPU offload enabled - parts of model will be on CPU")

            # Explicitn√≠ p≈ôesun na device
            if hasattr(model, 'to'):
                model.to(self.device)
            elif hasattr(model, 'model') and hasattr(model.model, 'to'):
                model.model.to(self.device)

            return model

        except Exception as e1:
            print(f"First attempt failed: {str(e1)}")
            # Fallback: zkus p≈ô√≠mo Hugging Face model
            try:
                print(f"Trying direct Hugging Face model: {XTTS_MODEL_NAME}")
                # Pou≈æijeme GPU pouze pokud je device nastaven na "cuda"
                use_gpu = (self.device == "cuda" and torch.cuda.is_available())
                model = TTS(
                    model_name=XTTS_MODEL_NAME,
                    progress_bar=True
                )
                if hasattr(model, 'to'):
                    model.to(self.device)
                elif hasattr(model, 'model') and hasattr(model.model, 'to'):
                    model.model.to(self.device)
                return model
            except Exception as e2:
                print(f"Both attempts failed. Error 1: {str(e1)}, Error 2: {str(e2)}")
                raise Exception(f"Failed to load model: {str(e2)}")

    def _apply_quality_preset(self, preset: str) -> dict:
        """
        Aplikuje quality preset na TTS parametry

        Args:
            preset: N√°zev presetu (high_quality, natural, fast, meditative, whisper)

        Returns:
            Slovn√≠k s TTS parametry
        """
        preset_config = QUALITY_PRESETS.get(preset, QUALITY_PRESETS["natural"])

        # Vr√°tit pouze TTS parametry (bez enhancement)
        tts_params = {
            "speed": preset_config.get("speed", 1.0),
            "temperature": preset_config.get("temperature", 0.7),
            "length_penalty": preset_config.get("length_penalty", 1.0),
            "repetition_penalty": preset_config.get("repetition_penalty", 2.0),
            "top_k": preset_config.get("top_k", 50),
            "top_p": preset_config.get("top_p", 0.85)
        }

        return tts_params

    def _compute_effective_settings(
        self,
        quality_mode: Optional[str] = None,
        enhancement_preset: Optional[str] = None,
        speed: Optional[float] = None,
        temperature: Optional[float] = None,
        length_penalty: Optional[float] = None,
        repetition_penalty: Optional[float] = None,
        top_k: Optional[int] = None,
        top_p: Optional[float] = None,
        enable_eq: Optional[bool] = None,
        enable_denoiser: Optional[bool] = None,
        enable_compressor: Optional[bool] = None,
        enable_deesser: Optional[bool] = None,
        enable_normalization: Optional[bool] = None,
        enable_trim: Optional[bool] = None,
        enable_whisper: Optional[bool] = None,
        whisper_intensity: Optional[float] = None,
        target_headroom_db: Optional[float] = None,
    ) -> dict:
        """
        Vypoƒç√≠t√° efektivn√≠ nastaven√≠ kombinac√≠ quality_mode presetu, enhancement_preset a explicitn√≠ch parametr≈Ø.

        Pravidla priority:
        1. Explicitn√≠ parametry (pokud zad√°ny) maj√≠ p≈ôednost p≈ôed presety
        2. quality_mode urƒçuje TTS parametry a enhancement (pokud je quality preset)
        3. enhancement_preset urƒçuje enhancement (pokud nen√≠ quality_mode nebo quality_mode nen√≠ quality preset)
        4. V√Ωchoz√≠ hodnoty z configu pro neexplicitn√≠ parametry

        Pro speed: Pokud je quality_mode in {meditative, whisper} a speed nen√≠ explicitnƒõ zad√°n,
        pou≈æije se speed z presetu. Jinak se zachov√° explicitn√≠ speed nebo v√Ωchoz√≠.

        Returns:
            Dictionary s efektivn√≠mi nastaven√≠mi:
            - tts: {speed, temperature, length_penalty, repetition_penalty, top_k, top_p}
            - enhancement: {enable_eq, enable_denoiser, enable_compressor, enable_deesser, enable_trim, enable_normalization}
            - whisper: {enable_whisper, whisper_intensity}
            - headroom: {target_headroom_db}
        """
        from backend.config import (
            TTS_SPEED, TTS_TEMPERATURE, TTS_LENGTH_PENALTY, TTS_REPETITION_PENALTY, TTS_TOP_K, TTS_TOP_P,
            OUTPUT_HEADROOM_DB, ENABLE_AUDIO_ENHANCEMENT
        )

        # V√Ωchoz√≠ hodnoty z configu
        defaults = {
            "speed": TTS_SPEED,
            "temperature": TTS_TEMPERATURE,
            "length_penalty": TTS_LENGTH_PENALTY,
            "repetition_penalty": TTS_REPETITION_PENALTY,
            "top_k": TTS_TOP_K,
            "top_p": TTS_TOP_P,
            "enable_eq": True,
            "enable_denoiser": True,
            "enable_compressor": True,
            "enable_deesser": True,
            "enable_trim": True,
            "enable_normalization": True,
            "enable_whisper": False,
            "whisper_intensity": 1.0,
            "target_headroom_db": OUTPUT_HEADROOM_DB,
        }

        # Naƒçti TTS parametry z quality_mode presetu (pokud existuje)
        preset_tts = {}
        preset_enhancement = {}
        if quality_mode and quality_mode in QUALITY_PRESETS:
            preset_config = QUALITY_PRESETS[quality_mode]
            preset_tts = self._apply_quality_preset(quality_mode)
            preset_enhancement = preset_config.get("enhancement", {})

        # Naƒçti enhancement z enhancement_preset (pokud je to quality preset a quality_mode nen√≠ nastaven)
        elif enhancement_preset and enhancement_preset in QUALITY_PRESETS:
            preset_config = QUALITY_PRESETS[enhancement_preset]
            preset_enhancement = preset_config.get("enhancement", {})

        # Sestav efektivn√≠ TTS parametry (explicitn√≠ > preset > v√Ωchoz√≠)
        effective_tts = {
            "speed": speed if speed is not None else (preset_tts.get("speed") if preset_tts else defaults["speed"]),
            "temperature": temperature if temperature is not None else (preset_tts.get("temperature") if preset_tts else defaults["temperature"]),
            "length_penalty": length_penalty if length_penalty is not None else (preset_tts.get("length_penalty") if preset_tts else defaults["length_penalty"]),
            "repetition_penalty": repetition_penalty if repetition_penalty is not None else (preset_tts.get("repetition_penalty") if preset_tts else defaults["repetition_penalty"]),
            "top_k": top_k if top_k is not None else (preset_tts.get("top_k") if preset_tts else defaults["top_k"]),
            "top_p": top_p if top_p is not None else (preset_tts.get("top_p") if preset_tts else defaults["top_p"]),
        }

        # Speci√°ln√≠ pravidlo pro speed: pokud je quality_mode meditative/whisper a speed nen√≠ explicitnƒõ zad√°n,
        # pou≈æij speed z presetu (pro meditative/whisper je to d≈Øle≈æit√© pro spr√°vn√Ω efekt)
        if quality_mode in ("meditative", "whisper") and speed is None:
            effective_tts["speed"] = preset_tts.get("speed", defaults["speed"])

        # Sestav efektivn√≠ enhancement parametry (explicitn√≠ > preset > v√Ωchoz√≠)
        # Mapov√°n√≠ n√°zv≈Ø: enable_noise_reduction -> enable_denoiser, enable_compression -> enable_compressor
        effective_enhancement = {
            "enable_eq": enable_eq if enable_eq is not None else (preset_enhancement.get("enable_eq", defaults["enable_eq"])),
            "enable_denoiser": enable_denoiser if enable_denoiser is not None else (preset_enhancement.get("enable_noise_reduction", defaults["enable_denoiser"])),
            "enable_compressor": enable_compressor if enable_compressor is not None else (preset_enhancement.get("enable_compression", defaults["enable_compressor"])),
            "enable_deesser": enable_deesser if enable_deesser is not None else (preset_enhancement.get("enable_deesser", defaults["enable_deesser"])),
            "enable_trim": enable_trim if enable_trim is not None else defaults["enable_trim"],
            "enable_normalization": enable_normalization if enable_normalization is not None else (preset_enhancement.get("enable_normalization", defaults["enable_normalization"])),
        }

        # Whisper efekt (z presetu nebo explicitn√≠)
        effective_whisper = {
            "enable_whisper": enable_whisper if enable_whisper is not None else (preset_enhancement.get("enable_whisper", defaults["enable_whisper"])),
            "whisper_intensity": whisper_intensity if whisper_intensity is not None else (preset_enhancement.get("whisper_intensity", defaults["whisper_intensity"])),
        }

        # Headroom (preset m≈Ø≈æe m√≠t target_headroom_db, jinak glob√°ln√≠)
        effective_headroom = {
            "target_headroom_db": target_headroom_db if target_headroom_db is not None else (preset_enhancement.get("target_headroom_db", defaults["target_headroom_db"])),
        }

        return {
            "tts": effective_tts,
            "enhancement": effective_enhancement,
            "whisper": effective_whisper,
            "headroom": effective_headroom,
        }

    async def generate(
        self,
        text: str,
        speaker_wav: str,
        language: str = "cs",
        speed: float = 1.0,
        temperature: float = 0.7,
        length_penalty: float = 1.0,
        repetition_penalty: float = 2.0,
        top_k: int = 50,
        top_p: float = 0.85,
        quality_mode: Optional[str] = None,
        seed: Optional[int] = None,
        enhancement_preset: Optional[str] = None,
        enable_enhancement: Optional[bool] = None,
        multi_pass: bool = False,
        multi_pass_count: int = 3,
        enable_batch: Optional[bool] = None,
        enable_vad: Optional[bool] = None,
        use_hifigan: bool = False,
        enable_normalization: bool = True,
        enable_denoiser: bool = True,
        enable_compressor: bool = True,
        enable_deesser: bool = True,
        enable_eq: bool = True,
        enable_trim: bool = True,
        handle_pauses: bool = True,
        enable_dialect_conversion: Optional[bool] = None,
        dialect_code: Optional[str] = None,
        dialect_intensity: float = 1.0,
        enable_whisper: Optional[bool] = None,
        whisper_intensity: Optional[float] = None,
        target_headroom_db: Optional[float] = None,
        hifigan_refinement_intensity: Optional[float] = None,
        hifigan_normalize_output: Optional[bool] = None,
        hifigan_normalize_gain: Optional[float] = None,
        job_id: Optional[str] = None
    ):
        """
        Generuje ≈ôeƒç z textu

        Args:
            text: Text k synt√©ze
            speaker_wav: Cesta k audio souboru s hlasem
            language: Jazyk (cs pro ƒçe≈°tinu)
            speed: Rychlost ≈ôeƒçi (0.5-2.0, v√Ωchoz√≠: 1.0)
            temperature: Teplota pro sampling (0.0-1.0, v√Ωchoz√≠: 0.7)
            length_penalty: Length penalty (v√Ωchoz√≠: 1.0)
            repetition_penalty: Repetition penalty (v√Ωchoz√≠: 2.0)
            top_k: Top-k sampling (v√Ωchoz√≠: 50)
            top_p: Top-p sampling (v√Ωchoz√≠: 0.85)
            quality_mode: Quality preset (high_quality, natural, fast) - p≈ôep√≠≈°e jednotliv√© parametry
            seed: Seed pro reprodukovatelnost generov√°n√≠ (voliteln√©)
            enhancement_preset: Preset pro audio enhancement (high_quality, natural, fast)
            multi_pass: Zapnout multi-pass generov√°n√≠ (v√Ωchoz√≠: False)
            multi_pass_count: Poƒçet variant p≈ôi multi-pass (v√Ωchoz√≠: 3)
            enable_batch: Zapnout batch processing pro dlouh√© texty (None = auto)
            enable_vad: Zapnout VAD pro lep≈°√≠ trim (None = pou≈æ√≠t config)
            use_hifigan: Pou≈æ√≠t HiFi-GAN vocoder (v√Ωchoz√≠: False)
            enable_normalization: Zapnout normalizaci (v√Ωchoz√≠: True)
            enable_denoiser: Zapnout redukci ≈°umu (v√Ωchoz√≠: True)
            enable_compressor: Zapnout kompresi (v√Ωchoz√≠: True)
            enable_deesser: Zapnout de-esser (v√Ωchoz√≠: True)
            enable_eq: Zapnout EQ (v√Ωchoz√≠: True)
            enable_trim: Zapnout o≈ôez ticha (v√Ωchoz√≠: True)

        Returns:
            Cesta k vygenerovan√©mu audio souboru nebo seznam variant p≈ôi multi-pass
        """
        if not self.is_loaded:
            await self.load_model()

        if not self.model:
            raise Exception("Model nen√≠ naƒçten")

        # Progress (pokud pou≈æ√≠v√°me job_id z frontendu)
        if job_id:
            try:
                from backend.progress_manager import ProgressManager
                ProgressManager.update(job_id, percent=2, stage="prepare", message="P≈ôipravuji generov√°n√≠‚Ä¶")
            except Exception:
                pass

        # Vypoƒç√≠tej efektivn√≠ nastaven√≠ (kombinace quality_mode, enhancement_preset a explicitn√≠ch parametr≈Ø)
        effective = self._compute_effective_settings(
            quality_mode=quality_mode,
            enhancement_preset=enhancement_preset,
            speed=speed,
            temperature=temperature,
            length_penalty=length_penalty,
            repetition_penalty=repetition_penalty,
            top_k=top_k,
            top_p=top_p,
            enable_eq=enable_eq,
            enable_denoiser=enable_denoiser,
            enable_compressor=enable_compressor,
            enable_deesser=enable_deesser,
            enable_normalization=enable_normalization,
            enable_trim=enable_trim,
            enable_whisper=enable_whisper,
            whisper_intensity=whisper_intensity,
            target_headroom_db=target_headroom_db,
        )

        # Extrahuj efektivn√≠ hodnoty
        speed = effective["tts"]["speed"]
        temperature = effective["tts"]["temperature"]
        length_penalty = effective["tts"]["length_penalty"]
        repetition_penalty = effective["tts"]["repetition_penalty"]
        top_k = effective["tts"]["top_k"]
        top_p = effective["tts"]["top_p"]
        enable_eq = effective["enhancement"]["enable_eq"]
        enable_denoiser = effective["enhancement"]["enable_denoiser"]
        enable_compressor = effective["enhancement"]["enable_compressor"]
        enable_deesser = effective["enhancement"]["enable_deesser"]
        enable_normalization = effective["enhancement"]["enable_normalization"]
        enable_trim = effective["enhancement"]["enable_trim"]
        enable_whisper = effective["whisper"]["enable_whisper"]
        whisper_intensity = effective["whisper"]["whisper_intensity"]
        target_headroom_db = effective["headroom"]["target_headroom_db"]

        if quality_mode:
            print(f"üéØ Quality mode '{quality_mode}' aplikov√°n - efektivn√≠ nastaven√≠ vypoƒç√≠t√°no z presetu (speed={speed:.2f}x)")

        # Multi-pass generov√°n√≠
        if multi_pass or (ENABLE_MULTI_PASS and not multi_pass):
            return await self.generate_multi_pass(
                text=text,
                speaker_wav=speaker_wav,
                language=language,
                speed=speed,
                temperature=temperature,
                length_penalty=length_penalty,
                repetition_penalty=repetition_penalty,
                top_k=top_k,
                top_p=top_p,
                quality_mode=quality_mode,
                enhancement_preset=enhancement_preset,
                variant_count=multi_pass_count if multi_pass else MULTI_PASS_COUNT,
                enable_batch=enable_batch,
                enable_vad=enable_vad,
                use_hifigan=use_hifigan,
                enable_normalization=enable_normalization,
                enable_denoiser=enable_denoiser,
                enable_compressor=enable_compressor,
                enable_deesser=enable_deesser,
                enable_eq=enable_eq,
                enable_trim=enable_trim,
                enable_whisper=enable_whisper,
                whisper_intensity=whisper_intensity,
                target_headroom_db=target_headroom_db,
                hifigan_refinement_intensity=hifigan_refinement_intensity,
                hifigan_normalize_output=hifigan_normalize_output,
                hifigan_normalize_gain=hifigan_normalize_gain,
                enable_enhancement=enable_enhancement,
                job_id=job_id
            )

        # Skuteƒçn√© pauzy: [PAUSE] / [pause] a [PAUSE:ms] / [pause:ms]
        # Pozn.: ProsodyProcessor historicky p≈ôev√°dƒõl pauzy jen na mezery (a p≈ôi batch splitu se ztrat√≠).
        # Tady to ≈ôe≈°√≠me spr√°vnƒõ: vygenerujeme √∫seky zvl√°≈°≈• a mezi nƒõ vlo≈æ√≠me ticho v milisekund√°ch.
        if handle_pauses:
            import re
            # Najdi v≈°echny pauzy a rozsekej text (case-insensitive).
            # Podporovan√© formy:
            # - [pause]
            # - [pause:500], [pause=500]
            # - [pause:500ms], [pause = 500 ms]
            pause_re = re.compile(r"\[pause(?:\s*[:=]\s*(\d+)\s*(?:ms)?)?\]", re.IGNORECASE)
            matches = list(pause_re.finditer(text))
            if matches:
                segments: List[str] = []
                pauses_ms: List[int] = []
                leading_pause_ms = 0
                last = 0
                pending_pause = 0

                for m in matches:
                    seg = text[last:m.start()]
                    dur_raw = m.group(1)
                    try:
                        dur = int(dur_raw) if dur_raw is not None else 500
                    except Exception:
                        dur = 500
                    dur = max(0, min(dur, 10000))  # 0‚Äì10s safety

                    # P≈ôidej segment (i pr√°zdn√Ω zat√≠m), pauzy sluƒçujeme pokud jsou za sebou
                    if seg.strip():
                        is_first_segment = len(segments) == 0
                        segments.append(seg.strip())
                        if pending_pause > 0:
                            # Pokud je≈°tƒõ nem√°me ≈æ√°dn√Ω segment, jde o pauzu NA ZAƒå√ÅTKU
                            if is_first_segment:
                                leading_pause_ms += pending_pause
                            else:
                                pauses_ms.append(pending_pause)
                            pending_pause = 0
                        pending_pause += dur
                    else:
                        pending_pause += dur

                    last = m.end()

                tail = text[last:]
                if tail.strip():
                    is_first_segment = len(segments) == 0
                    segments.append(tail.strip())
                    if pending_pause > 0:
                        if is_first_segment:
                            leading_pause_ms += pending_pause
                        else:
                            pauses_ms.append(pending_pause)
                        pending_pause = 0
                else:
                    # trailing pause bez dal≈°√≠ho textu: zachovej jako pauzu na konci
                    if pending_pause > 0 and segments:
                        pauses_ms.append(pending_pause)
                    pending_pause = 0

                # Pokud m√°me aspo≈à 2 segmenty, vygeneruj a spoj se skuteƒçn√Ωm tichem
                if len(segments) >= 2:
                    print(
                        f"‚è∏Ô∏è  Detekov√°ny pauzy v textu: {len(segments)} segment≈Ø, "
                        f"{len(pauses_ms)} pauz (vƒçetnƒõ p≈ô√≠padn√© pauzy na konci), "
                        f"leading_pause={leading_pause_ms}ms"
                    )
                    part_paths: List[str] = []
                    for idx, seg in enumerate(segments):
                        if job_id:
                            try:
                                from backend.progress_manager import ProgressManager
                                ProgressManager.update(
                                    job_id,
                                    percent=5 + (80.0 * idx / max(1, len(segments))),
                                    stage="pause_segments",
                                    message=f"Generuji segment {idx+1}/{len(segments)}‚Ä¶",
                                    meta_update={"segment": idx + 1, "segments_total": len(segments)},
                                )
                            except Exception:
                                pass
                        part_path = await self.generate(
                            text=seg,
                            speaker_wav=speaker_wav,
                            language=language,
                            speed=speed,
                            temperature=temperature,
                            length_penalty=length_penalty,
                            repetition_penalty=repetition_penalty,
                            top_k=top_k,
                            top_p=top_p,
                            quality_mode=quality_mode,
                            seed=seed,
                            enhancement_preset=enhancement_preset,
                            multi_pass=False,
                            # Batch uvnit≈ô segmentu je OK (segment s√°m neobsahuje [PAUSE]),
                            # a z√°rove≈à to chr√°n√≠ p≈ôed XTTS limitem 400 token≈Ø.
                            enable_batch=enable_batch,
                            enable_vad=enable_vad,
                            use_hifigan=use_hifigan,
                            enable_normalization=enable_normalization,
                            enable_denoiser=enable_denoiser,
                            enable_compressor=enable_compressor,
                            enable_deesser=enable_deesser,
                            enable_eq=enable_eq,
                            enable_trim=enable_trim,
                            handle_pauses=False,  # zabra≈à rekurzivn√≠mu parsov√°n√≠
                            job_id=job_id,
                        )
                        part_paths.append(part_path)

                    # Spoj WAVy + vlo≈æ ticho p≈ôesnƒõ podle ms
                    final_output = OUTPUTS_DIR / f"{uuid.uuid4()}.wav"
                    try:
                        if job_id:
                            try:
                                from backend.progress_manager import ProgressManager
                                ProgressManager.update(job_id, percent=90, stage="concat", message="Skl√°d√°m segmenty‚Ä¶")
                            except Exception:
                                pass
                        import librosa
                        import soundfile as sf

                        sr = OUTPUT_SAMPLE_RATE
                        # Kr√°tk√Ω fade proti "klik≈Øm". 8ms je u kr√°tk√Ωch pauz (10‚Äì50ms) moc a vizu√°lnƒõ je to m≈Ø≈æe "srovnat".
                        # Dr≈æ√≠me to mal√©, aby d√©lka pauz odpov√≠dala zadan√Ωm hodnot√°m.
                        fade_samples = int(0.001 * sr)  # 1 ms

                        out_parts: List[np.ndarray] = []
                        if leading_pause_ms > 0:
                            leading_samps = int(leading_pause_ms * sr / 1000)
                            print(f"‚è±Ô∏è  Leading pause: {leading_pause_ms} ms => {leading_samps} samples @ {sr} Hz")
                            out_parts.append(np.zeros(leading_samps, dtype=np.float32))
                        for i, p in enumerate(part_paths):
                            audio, _sr = librosa.load(p, sr=sr, mono=True)
                            # D≈ÆLE≈ΩIT√â: p≈ôi segmentaci na jednotliv√° slova model ƒçasto p≈ôid√° vlastn√≠ dlouh√© ticho
                            # na zaƒç√°tek/konec ka≈æd√©ho segmentu, tak≈æe pak v≈°echny pauzy zn√≠ stejnƒõ dlouh√©.
                            # Proto ka≈æd√Ω segment p≈ôed spojen√≠m o≈ô√≠zneme na ≈ôeƒç a nech√°me jen mal√Ω padding.
                            try:
                                from backend.vad_processor import get_vad_processor
                                vadp = get_vad_processor()
                                trimmed = vadp.trim_silence_vad(audio, sample_rate=sr, padding_ms=30.0)
                                if trimmed is not None and len(trimmed) > 0:
                                    audio = trimmed
                            except Exception:
                                # Fallback: energetick√Ω trim (m≈Ø≈æe b√Ωt m√©nƒõ p≈ôesn√Ω ne≈æ VAD)
                                try:
                                    audio, _ = librosa.effects.trim(audio, top_db=35)
                                except Exception:
                                    pass
                            # jemn√Ω fade in/out
                            if len(audio) > fade_samples * 2:
                                audio[:fade_samples] *= np.linspace(0.0, 1.0, fade_samples)
                                audio[-fade_samples:] *= np.linspace(1.0, 0.0, fade_samples)
                            out_parts.append(audio)

                            if i < len(pauses_ms):
                                pause_ms = pauses_ms[i]
                                pause_samps = int(pause_ms * sr / 1000)
                                if pause_samps > 0:
                                    print(f"‚è±Ô∏è  Pause[{i}]: {pause_ms} ms => {pause_samps} samples @ {sr} Hz")
                                    out_parts.append(np.zeros(pause_samps, dtype=np.float32))

                        final_audio = np.concatenate(out_parts) if out_parts else np.array([], dtype=np.float32)
                        sf.write(str(final_output), final_audio, sr)
                    finally:
                        # uklidit doƒçasn√© segmenty
                        for p in part_paths:
                            try:
                                Path(p).unlink(missing_ok=True)
                            except Exception:
                                pass

                    return str(final_output)

        # Batch processing pro dlouh√© texty
        hard_limit = getattr(config, "XTTS_MAX_TOKENS", 400)
        target_limit = getattr(config, "XTTS_TARGET_MAX_TOKENS", 380)
        token_count = self._count_xtts_tokens(text, language)

        # Pokud hroz√≠/u≈æ nastal token overflow, batch je povinn√Ω (jinak XTTS spadne).
        if token_count is not None and token_count > hard_limit:
            enable_batch = True

        use_batch = enable_batch if enable_batch is not None else (
            ENABLE_BATCH_PROCESSING and (
                (token_count is not None and token_count > target_limit) or (len(text) > MAX_CHUNK_LENGTH)
            )
        )
        if use_batch:
            return await self.generate_batch(
                text=text,
                speaker_wav=speaker_wav,
                language=language,
                speed=speed,
                temperature=temperature,
                length_penalty=length_penalty,
                repetition_penalty=repetition_penalty,
                top_k=top_k,
                top_p=top_p,
                quality_mode=quality_mode,
                seed=seed,
                enhancement_preset=enhancement_preset,
                enable_vad=enable_vad,
                use_hifigan=use_hifigan,
                enable_normalization=enable_normalization,
                enable_denoiser=enable_denoiser,
                enable_compressor=enable_compressor,
                enable_deesser=enable_deesser,
                enable_eq=enable_eq,
                enable_trim=enable_trim,
                enable_dialect_conversion=enable_dialect_conversion,
                dialect_code=dialect_code,
                dialect_intensity=dialect_intensity,
                job_id=job_id
            )

        # Prosody preprocessing
        prosody_metadata = {}
        try:
            from backend.prosody_processor import ProsodyProcessor
            if ENABLE_PROSODY_CONTROL:
                text, prosody_metadata = ProsodyProcessor.process_text(text)
        except Exception as e:
            print(f"Warning: Prosody processing failed: {e}")

        # Vytvo≈ôen√≠ v√Ωstupn√≠ cesty
        output_filename = f"{uuid.uuid4()}.wav"
        output_path = OUTPUTS_DIR / output_filename

        # Generov√°n√≠ v thread poolu
        if job_id:
            try:
                from backend.progress_manager import ProgressManager
                ProgressManager.update(job_id, percent=10, stage="synth", message="Syntetizuji‚Ä¶")
            except Exception:
                pass
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(
            None,
            self._generate_sync,
            text,
            speaker_wav,
            language,
            str(output_path),
            speed,
            temperature,
            length_penalty,
            repetition_penalty,
            top_k,
            top_p,
            quality_mode,
            seed,
            enhancement_preset,
            enable_vad,
            use_hifigan,
            enable_normalization,
            enable_denoiser,
            enable_compressor,
            enable_deesser,
            enable_eq,
            enable_trim,
            enable_dialect_conversion,
            dialect_code,
            dialect_intensity,
            enable_whisper,
            whisper_intensity,
            target_headroom_db,
            hifigan_refinement_intensity,
            hifigan_normalize_output,
            hifigan_normalize_gain,
            job_id,
            enable_enhancement,
            prosody_metadata,
        )

        # fin√°ln√≠ 100% ≈ôe≈°√≠ backend/main.py (ProgressManager.done(job_id))
        return str(output_path)

    def _generate_sync(
        self,
        text: str,
        speaker_wav: str,
        language: str,
        output_path: str,
        speed: float = 1.0,
        temperature: float = 0.7,
        length_penalty: float = 1.0,
        repetition_penalty: float = 2.0,
        top_k: int = 50,
        top_p: float = 0.85,
        quality_mode: Optional[str] = None,
        seed: Optional[int] = None,
        enhancement_preset: Optional[str] = None,
        enable_vad: Optional[bool] = None,
        use_hifigan: bool = False,
        enable_normalization: bool = True,
        enable_denoiser: bool = True,
        enable_compressor: bool = True,
        enable_deesser: bool = True,
        enable_eq: bool = True,
        enable_trim: bool = True,
        enable_dialect_conversion: Optional[bool] = None,
        dialect_code: Optional[str] = None,
        dialect_intensity: float = 1.0,
        enable_whisper: bool = False,
        whisper_intensity: float = 1.0,
        target_headroom_db: Optional[float] = None,
        hifigan_refinement_intensity: Optional[float] = None,
        hifigan_normalize_output: Optional[bool] = None,
        hifigan_normalize_gain: Optional[float] = None,
        job_id: Optional[str] = None,
        enable_enhancement: Optional[bool] = None,
        prosody_metadata: Optional[Dict] = None
    ):
        # DEBUG: Ovƒõ≈ôen√≠, ≈æe speed parametr skuteƒçnƒõ p≈ôich√°z√≠
        print(f"üîç DEBUG _generate_sync START: speed={speed}, type={type(speed)}, output_path={output_path}")
        """Synchronn√≠ generov√°n√≠ ≈ôeƒçi"""
        def _progress(pct: float, stage: str, msg: str):
            if not job_id:
                return
            try:
                from backend.progress_manager import ProgressManager
                ProgressManager.update(job_id, percent=pct, stage=stage, message=msg)
            except Exception:
                pass

        try:
            _progress(12, "prep", "P≈ôipravuji vstup‚Ä¶")
            # Nastaven√≠ seedu pro reprodukovatelnost
            if seed is not None:
                torch.manual_seed(seed)
                if torch.cuda.is_available():
                    torch.cuda.manual_seed_all(seed)
                np.random.seed(seed)
                print(f"üå± Seed nastaven na: {seed}")
            else:
                # Pokud nen√≠ seed zad√°n, pou≈æijeme fixn√≠ seed pro konzistenci
                fixed_seed = 42
                torch.manual_seed(fixed_seed)
                if torch.cuda.is_available():
                    torch.cuda.manual_seed_all(fixed_seed)
                np.random.seed(fixed_seed)
                print(f"üå± Pou≈æit fixn√≠ seed: {fixed_seed} (pro reprodukovatelnost)")

            # Zkontroluj, jestli speaker_wav existuje
            if not Path(speaker_wav).exists():
                raise Exception(f"Speaker audio file not found: {speaker_wav}")

            # P≈ôedzpracov√°n√≠ textu pro ƒçe≈°tinu - p≈ôevod ƒç√≠sel na slova
            # TTS knihovna m√° probl√©m s num2words pro ƒçe≈°tinu, tak≈æe p≈ôevedeme ƒç√≠sla ruƒçnƒõ
            from backend.cs_pipeline import preprocess_czech_text
            processed_text = preprocess_czech_text(
                text,
                language,
                enable_dialect_conversion=enable_dialect_conversion,
                dialect_code=dialect_code,
                dialect_intensity=dialect_intensity
            )

            # √öprava: Odstranit koncovou teƒçku jen pro XTTS model,
            # aby ji model nep≈ôeƒçetl jako slovo "teƒçka".
            # Intonace (FALL) je u≈æ zachycena v prosody_metadata z d≈ô√≠vƒõj≈°√≠ f√°ze.
            text_for_model = processed_text
            if language == "cs" and isinstance(text_for_model, str):
                # Odstran√≠me koncovou teƒçku/teƒçky a p≈ô√≠padn√© mezery za n√≠
                text_for_model = re.sub(r"\s*[.‚Ä¶]+(\s*)$", r"\1", text_for_model).rstrip()
                if not text_for_model.strip():
                    # Pokud by po odstranƒõn√≠ teƒçky nic nezbylo (nap≈ô. vstup "."),
                    # vr√°t√≠me p≈Øvodn√≠ text jako fallback
                    text_for_model = processed_text

            _progress(15, "tts", "Generuji ≈ôeƒç (XTTS)‚Ä¶")

            # P≈ô√≠prava parametr≈Ø pro tts_to_file
            # V≈ædy p≈ôed√°v√°me v≈°echny parametry, ne jen kdy≈æ se li≈°√≠ od v√Ωchoz√≠ch hodnot
            # POZN√ÅMKA: XTTS-v2 nemus√≠ podporovat parametr "speed" p≈ô√≠mo v tts_to_file,
            # tak≈æe zmƒõnu rychlosti prov√°d√≠me pomoc√≠ post-processing (viz n√≠≈æe)

            # Validace a korekce extr√©mn√≠ch parametr≈Ø, kter√© mohou zp≈Øsobovat probl√©my
            # Extr√©mnƒõ n√≠zk√° temperature (< 0.2) m≈Ø≈æe zp≈Øsobovat chrƒçen√≠ a dlouh√© ticho
            safe_temperature = max(0.3, min(1.0, temperature)) if temperature < 0.3 else temperature
            if safe_temperature != temperature:
                print(f"‚ö†Ô∏è Temperature {temperature} je p≈ô√≠li≈° n√≠zk√°, upravuji na {safe_temperature} (min: 0.3)")

            # Extr√©mnƒõ vysok√° length_penalty (> 1.5) m≈Ø≈æe zp≈Øsobovat velmi dlouh√© generov√°n√≠
            safe_length_penalty = min(1.3, max(0.5, length_penalty)) if length_penalty > 1.3 else length_penalty
            if safe_length_penalty != length_penalty:
                print(f"‚ö†Ô∏è Length penalty {length_penalty} je p≈ô√≠li≈° vysok√°, upravuji na {safe_length_penalty} (max: 1.3)")

            # Extr√©mnƒõ n√≠zk√° repetition_penalty (< 1.3) m≈Ø≈æe zp≈Øsobovat opakov√°n√≠
            safe_repetition_penalty = max(1.5, min(3.0, repetition_penalty)) if repetition_penalty < 1.5 else repetition_penalty
            if safe_repetition_penalty != repetition_penalty:
                print(f"‚ö†Ô∏è Repetition penalty {repetition_penalty} je p≈ô√≠li≈° n√≠zk√°, upravuji na {safe_repetition_penalty} (min: 1.5)")

            # Extr√©mnƒõ n√≠zk√° top_p (< 0.3) m≈Ø≈æe zp≈Øsobovat probl√©my
            safe_top_p = max(0.5, min(0.95, top_p)) if top_p < 0.5 else top_p
            if safe_top_p != top_p:
                print(f"‚ö†Ô∏è Top-p {top_p} je p≈ô√≠li≈° n√≠zk√°, upravuji na {safe_top_p} (min: 0.5)")

            tts_params = {
                "text": text_for_model,
                "speaker_wav": speaker_wav,
                "language": language,
                "file_path": output_path,
                # speed se nep≈ôed√°v√° - pou≈æijeme post-processing m√≠sto toho
                "temperature": safe_temperature,
                "length_penalty": safe_length_penalty,
                "repetition_penalty": safe_repetition_penalty,
                "top_k": top_k,
                "top_p": safe_top_p
            }

            # Voliteln√©: pou≈æ√≠t caching conditioning latents (pokud to verze TTS podporuje)
            # C√≠l: rychlej≈°√≠ opakovan√© generov√°n√≠ + stabilnƒõj≈°√≠ conditioning u stejn√©ho referenƒçn√≠ho hlasu.
            try:
                from backend.config import ENABLE_SPEAKER_CACHE
                if ENABLE_SPEAKER_CACHE and self.model is not None:
                    import inspect
                    from backend.speaker_adapter import get_speaker_adapter

                    sig = None
                    try:
                        sig = inspect.signature(self.model.tts_to_file)
                    except Exception:
                        sig = None

                    if sig is not None:
                        param_names = set(sig.parameters.keys())
                        supports_embed = ("speaker_embeddings" in param_names) or ("speaker_embedding" in param_names)
                        supports_gpt = ("gpt_cond_latent" in param_names) or ("gpt_cond_latents" in param_names)

                        if supports_embed:
                            adapter = get_speaker_adapter()
                            latents = adapter.get_conditioning_latents(speaker_wav, self.model)
                            if latents is not None:
                                gpt_cond_latent, speaker_embedding = latents
                                # P≈ôesu≈à na spr√°vn√Ω device, pokud je pot≈ôeba
                                try:
                                    device = self.device
                                    if hasattr(gpt_cond_latent, "to"):
                                        gpt_cond_latent = gpt_cond_latent.to(device)
                                    if hasattr(speaker_embedding, "to"):
                                        speaker_embedding = speaker_embedding.to(device)
                                except Exception:
                                    pass

                                # Preferuj embeddingy m√≠sto speaker_wav (aby se conditioning znovu nepoƒç√≠tal)
                                tts_params.pop("speaker_wav", None)
                                if "speaker_embeddings" in param_names:
                                    tts_params["speaker_embeddings"] = speaker_embedding
                                elif "speaker_embedding" in param_names:
                                    tts_params["speaker_embedding"] = speaker_embedding

                                if supports_gpt:
                                    if "gpt_cond_latent" in param_names:
                                        tts_params["gpt_cond_latent"] = gpt_cond_latent
                                    elif "gpt_cond_latents" in param_names:
                                        tts_params["gpt_cond_latents"] = gpt_cond_latent
            except Exception as e:
                print(f"‚ö†Ô∏è Conditioning cache nepou≈æit (ignorov√°no): {e}")

            # Logov√°n√≠ parametr≈Ø pro debug
            print(f"üîä TTS Generation Parameters:")
            print(f"   Speed: {speed}")
            print(f"   Temperature: {temperature}")
            print(f"   Length Penalty: {length_penalty}")
            print(f"   Repetition Penalty: {repetition_penalty}")
            print(f"   Top-K: {top_k}")
            print(f"   Top-P: {top_p}")
            print(f"   Quality Mode: {quality_mode if quality_mode else 'None (using individual params)'}")

            # Heartbeat mechanismus bƒõhem XTTS inference (uk√°≈æe, ≈æe proces st√°le bƒõ≈æ√≠)
            heartbeat_stop = threading.Event()
            heartbeat_pct = [15.0]  # mutable pro thread

            def heartbeat_worker():
                """Aktualizuje progress pravidelnƒõ bƒõhem inference"""
                import time
                # Odhad rychlosti: cca 15 znak≈Ø za sekundu na pr≈Ømƒõrn√©m stroji
                # Pro 150 znak≈Ø (cca 10s) chceme doj√≠t z 15% na 50% (+35%)
                char_count = len(text)
                estimated_seconds = max(3.0, char_count / 15.0)
                # Kolik procent p≈ôidat ka≈æd√Ωch 0.5 sekundy
                increment = (35.0 / (estimated_seconds * 2.0))

                while not heartbeat_stop.is_set():
                    time.sleep(0.5)
                    if heartbeat_stop.is_set():
                        break
                    # Postupnƒõ zvy≈°uj progress (15% ‚Üí 55% bƒõhem inference)
                    # ƒåastƒõj≈°√≠ mal√© updaty + CSS transition na FE vytvo≈ô√≠ plynul√Ω pohyb
                    heartbeat_pct[0] = min(55.0, heartbeat_pct[0] + increment)
                    _progress(heartbeat_pct[0], "tts", f"Generuji ≈ôeƒç‚Ä¶ ({int(heartbeat_pct[0])}%)")

            heartbeat_thread = None
            if job_id:
                heartbeat_thread = threading.Thread(target=heartbeat_worker, daemon=True)
                heartbeat_thread.start()

            try:
                # Generov√°n√≠ ≈ôeƒçi
                # XTTS-v2 podporuje tyto parametry p≈ô√≠mo v tts_to_file:
                # - temperature: Teplota pro sampling (0.0-1.0)
                # - length_penalty: Length penalty (0.5-2.0)
                # - repetition_penalty: Repetition penalty (1.0-5.0)
                # - top_k: Top-k sampling (1-100)
                # - top_p: Top-p sampling (0.0-1.0)
                # POZN√ÅMKA: speed se nep≈ôed√°v√° - pou≈æijeme post-processing m√≠sto toho
                # Pokud nƒõkter√Ω parametr nen√≠ podporov√°n, XTTS ho ignoruje nebo vyhod√≠ TypeError
                try:
                    result = self.model.tts_to_file(**tts_params)
                except TypeError as e:
                    # Pokud nƒõkter√Ω parametr nen√≠ podporov√°n, zkus√≠me bez voliteln√Ωch parametr≈Ø
                    error_msg = str(e)
                    print(f"‚ö†Ô∏è Warning: Some parameters may not be supported: {error_msg}")
                    print("   Attempting with basic parameters only (temperature)...")

                    # Z√°kladn√≠ parametry + pouze temperature (nejƒçastƒõji podporovan√©)
                    basic_params = {
                        # Pou≈æij stejn√Ω text jako pro hlavn√≠ inference, aby se na konci neƒçetla "teƒçka"
                        "text": text_for_model,
                        "speaker_wav": speaker_wav,
                        "language": language,
                        "file_path": output_path,
                        "temperature": temperature
                    }

                    result = self.model.tts_to_file(**basic_params)
                    print("   ‚ö†Ô∏è Note: Some advanced parameters (length_penalty, repetition_penalty, top_k, top_p) may not be supported by this XTTS version")
            finally:
                # Zastav heartbeat
                if heartbeat_thread:
                    heartbeat_stop.set()
                    heartbeat_thread.join(timeout=1.0)

            # Zkontroluj, jestli soubor byl vytvo≈ôen
            if not Path(output_path).exists():
                raise Exception(f"Output file was not created: {output_path}")

            _progress(55, "tts", "XTTS inference dokonƒçeno")

            _progress(58, "upsample", "Naƒç√≠t√°m audio‚Ä¶")
            # Post-processing: trimov√°n√≠ P≈òED upsamplingem (odstran√≠ ticho a artefakty d≈ô√≠ve)
            # XTTS-v2 generuje na 22050-24000 Hz, ale chceme CD kvalitu (44100 Hz)
            try:
                import librosa
                import soundfile as sf

                # Naƒçten√≠ audio s p≈Øvodn√≠ sample rate
                audio, sr = librosa.load(output_path, sr=None)
                original_length = len(audio) / sr

                # TRIMOV√ÅN√ç P≈òED UPSAMPLINGEM - d≈Øle≈æit√© pro odstranƒõn√≠ ticha a artefakt≈Ø
                # Pro kr√°tk√© texty pou≈æij agresivnƒõj≈°√≠ trim
                word_count = len(text.split())
                is_short_text = word_count <= 3

                if is_short_text or original_length > 10.0:
                    try:
                        from backend.vad_processor import get_vad_processor
                        from backend.config import ENABLE_VAD

                        if ENABLE_VAD:
                            vad_processor = get_vad_processor()
                            padding = 20.0 if is_short_text else 50.0
                            audio_trimmed = vad_processor.trim_silence_vad(
                                audio,
                                sample_rate=sr,
                                padding_ms=padding
                            )
                            if audio_trimmed is not None and len(audio_trimmed) > 0:
                                audio = audio_trimmed
                                print(f"‚úÇÔ∏è VAD trim (p≈ôed upsamplingem): {original_length:.1f}s ‚Üí {len(audio)/sr:.1f}s")
                        else:
                            # Fallback: agresivnƒõj≈°√≠ librosa trim
                            top_db = 40 if is_short_text else 30
                            audio, _ = librosa.effects.trim(audio, top_db=top_db, frame_length=2048, hop_length=512)
                            print(f"‚úÇÔ∏è Librosa trim (p≈ôed upsamplingem): {original_length:.1f}s ‚Üí {len(audio)/sr:.1f}s")
                    except Exception as e:
                        # Fallback: agresivnƒõj≈°√≠ librosa trim
                        top_db = 40 if is_short_text else 30
                        audio, _ = librosa.effects.trim(audio, top_db=top_db, frame_length=2048, hop_length=512)
                        print(f"‚úÇÔ∏è Fallback trim (p≈ôed upsamplingem): {original_length:.1f}s ‚Üí {len(audio)/sr:.1f}s")

                # Maxim√°ln√≠ d√©lka pro kr√°tk√© texty (p≈ôed upsamplingem)
                if is_short_text:
                    max_duration_samples = int(5.0 * sr)
                    if len(audio) > max_duration_samples:
                        print(f"‚ö†Ô∏è Kr√°tk√Ω text ({word_count} slova) je p≈ô√≠li≈° dlouh√Ω ({len(audio)/sr:.1f}s), o≈ôez√°v√°m na 5s")
                        audio = audio[:max_duration_samples]

                # Upsampling na c√≠lovou sample rate (pokud je jin√°)
                if sr != OUTPUT_SAMPLE_RATE:
                    _progress(62, "upsample", f"P≈ôevzorkov√°n√≠ z {sr} Hz na {OUTPUT_SAMPLE_RATE} Hz‚Ä¶")
                    print(f"üéµ Upsampling audio z {sr} Hz na {OUTPUT_SAMPLE_RATE} Hz (CD kvalita)...")
                    audio = librosa.resample(audio, orig_sr=sr, target_sr=OUTPUT_SAMPLE_RATE)
                    sr = OUTPUT_SAMPLE_RATE
                    print(f"‚úÖ Audio upsamplov√°no na {OUTPUT_SAMPLE_RATE} Hz")

                # Prosody post-processing (intonace a emphasis) - p≈ôed enhancement
                if prosody_metadata:
                    try:
                        from backend.intonation_processor import IntonationProcessor
                        from backend.audio_enhancer import AudioEnhancer
                        from backend.config import ENABLE_INTONATION_PROCESSING, ENABLE_PROSODY_CONTROL

                        # Intonaƒçn√≠ post-processing
                        if ENABLE_INTONATION_PROCESSING and prosody_metadata.get('intonation'):
                            _progress(63, "intonation", "Aplikuji intonaci‚Ä¶")
                            intonation_metadata = prosody_metadata.get('intonation', [])

                            if intonation_metadata:
                                print(f"üéµ Aplikuji {len(intonation_metadata)} intonaƒçn√≠ch zmƒõn‚Ä¶")
                                applied_count = 0

                                for i, inton in enumerate(intonation_metadata):
                                    inton_type = inton.get('intonation_type')
                                    contour = inton.get('contour')
                                    content = inton.get('content', '')
                                    position = inton.get('position', 0)
                                    length = inton.get('length', len(content))
                                    intensity = inton.get('intensity', 1.0)
                                    auto_detected = inton.get('auto_detected', False)

                                    print(f"   Intonace {i+1}: type={inton_type}, auto={auto_detected}, content='{content[:50]}', pos={position}, len={length}")

                                    # Vypoƒç√≠tej pozice v audio (relativn√≠ k p≈Øvodn√≠mu textu)
                                    text_length = len(text)
                                    if text_length > 0:
                                        start_ratio = position / text_length
                                        end_ratio = (position + length) / text_length

                                        start_sample = int(start_ratio * len(audio))
                                        end_sample = int(end_ratio * len(audio))

                                        print(f"      Audio: text_len={text_length}, audio_len={len(audio)}, start={start_sample}, end={end_sample}")

                                        if start_sample < end_sample and end_sample <= len(audio):
                                            if contour:
                                                # Aplikuj konturu
                                                segment = audio[start_sample:end_sample]
                                                modified_segment = IntonationProcessor.apply_contour(
                                                    segment, sr, contour
                                                )
                                                audio[start_sample:end_sample] = modified_segment
                                                applied_count += 1
                                                print(f"      ‚úÖ Kontura aplikov√°na na segment {start_sample}-{end_sample}")
                                            elif inton_type:
                                                # Pro automaticky detekovanou intonaci
                                                if auto_detected and inton_type in ['FALL', 'RISE', 'HALF_FALL']:
                                                    segment_length = end_sample - start_sample

                                                    # Pro FALL aplikuj na celou vƒõtu pro v√Ωraznƒõj≈°√≠ a p≈ôirozenƒõj≈°√≠ pokles
                                                    if inton_type == 'FALL':
                                                        # Aplikuj FALL na celou vƒõtu - profil u≈æ m√° pokles jen na konci
                                                        intonation_start = start_sample
                                                        intonation_end = end_sample

                                                        # Zkontroluj, jestli je to vyk≈ôiƒçn√≠k (vy≈°≈°√≠ intenzita)
                                                        is_exclamation = inton.get('is_exclamation', False)
                                                        if is_exclamation:
                                                            # Pro vyk≈ôiƒçn√≠k pou≈æij m√≠rnƒõ vy≈°≈°√≠ intenzitu
                                                            fall_intensity = 1.2
                                                            print(f"      Auto-detekce FALL (vyk≈ôiƒçn√≠k!): aplikuji na celou vƒõtu ({intonation_start}-{intonation_end}, 100% vƒõty, intensity={fall_intensity})")
                                                        else:
                                                            # P≈ôirozen√° intenzita pro bƒõ≈æn√Ω FALL
                                                            fall_intensity = 1.1
                                                            print(f"      Auto-detekce FALL: aplikuji na celou vƒõtu ({intonation_start}-{intonation_end}, 100% vƒõty, intensity={fall_intensity})")
                                                    elif inton_type == 'RISE':
                                                        # Pro RISE staƒç√≠ posledn√≠ch 40% - stoup√°n√≠ je v√Ωraznƒõj≈°√≠
                                                        intonation_start = start_sample + int(segment_length * 0.6)
                                                        intonation_end = end_sample
                                                        fall_intensity = intensity
                                                        print(f"      Auto-detekce RISE: aplikuji na konec ({intonation_start}-{intonation_end}, {100*(intonation_end-intonation_start)/segment_length:.0f}% vƒõty)")
                                                    else:  # HALF_FALL
                                                        intonation_start = start_sample + int(segment_length * 0.7)
                                                        intonation_end = end_sample
                                                        fall_intensity = intensity
                                                        print(f"      Auto-detekce HALF_FALL: aplikuji na konec ({intonation_start}-{intonation_end}, {100*(intonation_end-intonation_start)/segment_length:.0f}% vƒõty)")

                                                    if intonation_start < intonation_end:
                                                        audio = IntonationProcessor.apply_intonation_to_segment(
                                                            audio, sr, intonation_start, intonation_end,
                                                            inton_type, fall_intensity if inton_type == 'FALL' else intensity
                                                        )
                                                        applied_count += 1
                                                        print(f"      ‚úÖ Intonace {inton_type} aplikov√°na na segment {intonation_start}-{intonation_end}")
                                                    else:
                                                        # Pokud je segment p≈ô√≠li≈° kr√°tk√Ω, aplikuj na cel√Ω
                                                        audio = IntonationProcessor.apply_intonation_to_segment(
                                                            audio, sr, start_sample, end_sample,
                                                            inton_type, fall_intensity if inton_type == 'FALL' else intensity
                                                        )
                                                        applied_count += 1
                                                        print(f"      ‚úÖ Intonace {inton_type} aplikov√°na na cel√Ω segment {start_sample}-{end_sample} (segment p≈ô√≠li≈° kr√°tk√Ω)")
                                                else:
                                                    # Pro explicitn√≠ znaƒçky aplikuj na cel√Ω segment
                                                    audio = IntonationProcessor.apply_intonation_to_segment(
                                                        audio, sr, start_sample, end_sample,
                                                        inton_type, intensity
                                                    )
                                                    applied_count += 1
                                                    print(f"      ‚úÖ Intonace {inton_type} aplikov√°na na segment {start_sample}-{end_sample}")
                                        else:
                                            print(f"      ‚ö†Ô∏è Intonace NEN√ç aplikov√°na: start={start_sample}, end={end_sample}, audio_len={len(audio)}")

                                print(f"‚úÖ Intonace aplikov√°na: {applied_count}/{len(intonation_metadata)}")

                        # Emphasis post-processing
                        if ENABLE_PROSODY_CONTROL and prosody_metadata.get('emphasis'):
                            _progress(64, "emphasis", "Aplikuji d≈Øraz‚Ä¶")
                            emphasis_metadata = prosody_metadata.get('emphasis', [])

                            if emphasis_metadata:
                                print(f"üí™ Aplikuji {len(emphasis_metadata)} d≈Øraz≈Ø‚Ä¶")
                                applied_count = 0

                                for i, emph in enumerate(emphasis_metadata):
                                    level = emph.get('level', 'MODERATE')
                                    # Pou≈æij zpracovan√Ω obsah a pozici (pokud existuje), jinak p≈Øvodn√≠
                                    processed_content = emph.get('processed_content', emph.get('content', ''))
                                    processed_position = emph.get('processed_position', emph.get('position', 0))
                                    processed_length = emph.get('processed_length', len(processed_content))
                                    auto_detected_emphasis = emph.get('auto_detected', False)

                                    print(f"   Emphasis {i+1}: level={level}, auto={auto_detected_emphasis}, content='{emph.get('content', '')[:30]}', processed='{processed_content[:30]}', pos={processed_position}, len={processed_length}")

                                    # Vypoƒç√≠tej pozice v audio (relativn√≠ k zpracovan√©mu textu)
                                    text_length = len(text)
                                    if text_length > 0:
                                        # Odhad d√©lky emphasis segmentu v audio
                                        if processed_length > 0:
                                            content_ratio = processed_length / text_length
                                            segment_length = int(content_ratio * len(audio))

                                            # Najdi pozici v audio
                                            position_ratio = processed_position / text_length
                                            start_sample = int(position_ratio * len(audio))
                                            end_sample = min(start_sample + segment_length, len(audio))

                                            print(f"      Audio: text_len={text_length}, audio_len={len(audio)}, start={start_sample}, end={end_sample}, segment_len={segment_length}")

                                            if start_sample < end_sample and end_sample <= len(audio):
                                                # Aplikuj emphasis efekt na segment (zv√Ω≈°en√° intenzita pro v√Ωraznƒõj≈°√≠ efekt)
                                                segment = audio[start_sample:end_sample]
                                                # Pro STRONG pou≈æij vy≈°≈°√≠ intenzitu (1.5), pro MODERATE standardn√≠ (1.0)
                                                # Pro automaticky detekovan√Ω emphasis z vyk≈ôiƒçn√≠ku pou≈æij je≈°tƒõ vy≈°≈°√≠ intenzitu
                                                if auto_detected_emphasis and emph.get('source') == 'exclamation':
                                                    # Bezpeƒçn√Ω d≈Øraz pro vyk≈ôiƒçn√≠k (bez p≈ôebuzen√≠)
                                                    emphasis_intensity = 1.15
                                                elif level == 'STRONG':
                                                    emphasis_intensity = 1.5
                                                else:
                                                    emphasis_intensity = 1.0
                                                modified_segment = AudioEnhancer.apply_emphasis_effect(
                                                    segment, sr, level=level, intensity=emphasis_intensity
                                                )

                                                # Vlo≈æ zpƒõt s vyhlazen√≠m p≈ôechod≈Ø
                                                fade_samples = min(int(sr * 0.01), len(modified_segment) // 4)  # 10ms fade
                                                if fade_samples > 0:
                                                    fade_in = np.linspace(0.0, 1.0, fade_samples)
                                                    fade_out = np.linspace(1.0, 0.0, fade_samples)
                                                    modified_segment[:fade_samples] *= fade_in
                                                    modified_segment[-fade_samples:] *= fade_out

                                                audio[start_sample:end_sample] = modified_segment
                                                applied_count += 1
                                                print(f"      ‚úÖ Emphasis aplikov√°n na segment {start_sample}-{end_sample}")
                                            else:
                                                print(f"      ‚ö†Ô∏è Emphasis NEN√ç aplikov√°n: start={start_sample}, end={end_sample}, audio_len={len(audio)}")
                                        else:
                                            print(f"      ‚ö†Ô∏è Emphasis NEN√ç aplikov√°n: processed_length=0")
                                    else:
                                        print(f"      ‚ö†Ô∏è Emphasis NEN√ç aplikov√°n: text_length=0")

                                print(f"‚úÖ D≈Øraz aplikov√°n: {applied_count}/{len(emphasis_metadata)}")

                        # Rate post-processing (rychlost ≈ôeƒçi)
                        if ENABLE_PROSODY_CONTROL and prosody_metadata.get('rate_changes'):
                            _progress(65, "rate", "Aplikuji rychlost‚Ä¶")
                            rate_metadata = prosody_metadata.get('rate_changes', [])

                            if rate_metadata:
                                print(f"‚ö° Aplikuji {len(rate_metadata)} zmƒõn rychlosti‚Ä¶")

                                for rate_info in rate_metadata:
                                    rate = rate_info.get('rate', 'NORMAL')
                                    content = rate_info.get('content', '')
                                    position = rate_info.get('position', 0)

                                    # Vypoƒç√≠tej pozice v audio
                                    text_length = len(text)
                                    if text_length > 0:
                                        content_length = len(content)
                                        if content_length > 0:
                                            content_ratio = content_length / text_length
                                            segment_length = int(content_ratio * len(audio))

                                            position_ratio = position / text_length
                                            start_sample = int(position_ratio * len(audio))
                                            end_sample = min(start_sample + segment_length, len(audio))

                                            if start_sample < end_sample and end_sample <= len(audio):
                                                # Aplikuj rate efekt na segment
                                                segment = audio[start_sample:end_sample]
                                                modified_segment = AudioEnhancer.apply_rate_effect(
                                                    segment, sr, rate=rate, intensity=1.0
                                                )

                                                # Vlo≈æ zpƒõt s vyhlazen√≠m p≈ôechod≈Ø
                                                fade_samples = min(int(sr * 0.01), len(modified_segment) // 4)
                                                if fade_samples > 0:
                                                    fade_in = np.linspace(0.0, 1.0, fade_samples)
                                                    fade_out = np.linspace(1.0, 0.0, fade_samples)
                                                    modified_segment[:fade_samples] *= fade_in
                                                    modified_segment[-fade_samples:] *= fade_out

                                                # Pokud se d√©lka zmƒõnila, mus√≠me upravit audio
                                                length_diff = len(modified_segment) - len(segment)
                                                if length_diff != 0:
                                                    # Vytvo≈ô nov√© audio s upravenou d√©lkou
                                                    new_audio = np.zeros(len(audio) + length_diff, dtype=audio.dtype)
                                                    new_audio[:start_sample] = audio[:start_sample]
                                                    new_audio[start_sample:start_sample + len(modified_segment)] = modified_segment
                                                    new_audio[start_sample + len(modified_segment):] = audio[end_sample:]
                                                    audio = new_audio
                                                else:
                                                    audio[start_sample:end_sample] = modified_segment

                                print(f"‚úÖ Rychlost aplikov√°na")

                        # Pitch post-processing (v√Ω≈°ka hlasu)
                        if ENABLE_PROSODY_CONTROL and prosody_metadata.get('pitch_changes'):
                            _progress(66, "pitch", "Aplikuji v√Ω≈°ku hlasu‚Ä¶")
                            pitch_metadata = prosody_metadata.get('pitch_changes', [])

                            if pitch_metadata:
                                print(f"üéµ Aplikuji {len(pitch_metadata)} zmƒõn v√Ω≈°ky hlasu‚Ä¶")

                                for pitch_info in pitch_metadata:
                                    pitch = pitch_info.get('pitch', 'NORMAL')
                                    content = pitch_info.get('content', '')
                                    position = pitch_info.get('position', 0)

                                    # Vypoƒç√≠tej pozice v audio
                                    text_length = len(text)
                                    if text_length > 0:
                                        content_length = len(content)
                                        if content_length > 0:
                                            content_ratio = content_length / text_length
                                            segment_length = int(content_ratio * len(audio))

                                            position_ratio = position / text_length
                                            start_sample = int(position_ratio * len(audio))
                                            end_sample = min(start_sample + segment_length, len(audio))

                                            if start_sample < end_sample and end_sample <= len(audio):
                                                # Aplikuj pitch efekt na segment
                                                segment = audio[start_sample:end_sample]
                                                modified_segment = AudioEnhancer.apply_pitch_effect(
                                                    segment, sr, pitch=pitch, intensity=1.0
                                                )

                                                # Vlo≈æ zpƒõt s vyhlazen√≠m p≈ôechod≈Ø
                                                fade_samples = min(int(sr * 0.01), len(modified_segment) // 4)
                                                if fade_samples > 0:
                                                    fade_in = np.linspace(0.0, 1.0, fade_samples)
                                                    fade_out = np.linspace(1.0, 0.0, fade_samples)
                                                    modified_segment[:fade_samples] *= fade_in
                                                    modified_segment[-fade_samples:] *= fade_out

                                                audio[start_sample:end_sample] = modified_segment

                                print(f"‚úÖ V√Ω≈°ka hlasu aplikov√°na")
                    except Exception as e:
                        print(f"‚ö†Ô∏è Warning: Prosody post-processing selhal: {e}")

                # Ulo≈æen√≠ s upsamplovan√Ωm audio (p≈ôed enhancement)
                sf.write(output_path, audio, sr)
                _progress(65, "upsample", "Upsampling dokonƒçen")

            except Exception as e:
                print(f"‚ö†Ô∏è Warning: Post-processing (upsampling) failed: {e}, continuing with original audio")
                # Pokraƒçujeme s p≈Øvodn√≠m audio

            # Post-processing audio enhancement (pokud je zapnuto)
            if ENABLE_AUDIO_ENHANCEMENT and (enable_enhancement is None or enable_enhancement):
                try:
                    # Pou≈æ√≠t p≈ôedan√Ω enhancement_preset, nebo v√Ωchoz√≠ z configu (pro kompatibilitu se star√Ωm k√≥dem)
                    preset_to_use = enhancement_preset if enhancement_preset else AUDIO_ENHANCEMENT_PRESET

                    # Progress callback wrapper: mapuje 0-100 z AudioEnhancer na 68-88 v celkov√©m progressu
                    def enhance_progress(percent: float, stage: str, message: str):
                        mapped_percent = 68.0 + (percent / 100.0) * 20.0  # 68-88%
                        _progress(mapped_percent, "enhance", message)

                    # Vol√°n√≠ jednotn√© enhancement metody
                    AudioEnhancer.enhance_output(
                        audio_path=str(output_path),
                        preset=preset_to_use,
                        enable_eq=enable_eq,
                        enable_noise_reduction=enable_denoiser,
                        enable_compression=enable_compressor,
                        enable_deesser=enable_deesser,
                        enable_normalization=enable_normalization,
                        enable_trim=enable_trim,
                        enable_whisper=enable_whisper,
                        whisper_intensity=whisper_intensity,
                        enable_vad=enable_vad,
                        target_headroom_db=target_headroom_db,
                        progress_callback=enhance_progress
                    )
                except Exception as e:
                    print(f"Warning: Audio enhancement failed: {e}, continuing with original audio")
                    _progress(88, "enhance", "Enhancement p≈ôeskoƒçen (chyba)")

            # HiFi-GAN Vocoder refinement (pokud zapnuto)
            # POZN√ÅMKA: Mus√≠ b√Ωt p≈ôed zmƒõnou rychlosti, aby speed nebyl p≈ôeps√°n
            if use_hifigan and self.vocoder.is_available():
                try:
                    _progress(93, "hifigan", "HiFi-GAN refinement‚Ä¶")
                    import librosa
                    import soundfile as sf

                    print("üöÄ Aplikuji HiFi-GAN vocoder refinement...")
                    # Naƒçten√≠ aktu√°ln√≠ho audio
                    audio, sr = librosa.load(output_path, sr=None)
                    original_audio = audio.copy()  # Ulo≈æit pro p≈ô√≠padn√© blending

                    # 1. V√Ωpoƒçet mel-spectrogramu z vygenerovan√©ho audio
                    # Pou≈æijeme parametry z configu
                    mel_params = self.vocoder.mel_params
                    mel = librosa.feature.melspectrogram(
                        y=audio,
                        sr=sr,
                        n_fft=mel_params["n_fft"],
                        hop_length=mel_params["hop_length"],
                        win_length=mel_params["win_length"],
                        n_mels=mel_params["n_mels"],
                        fmin=mel_params["fmin"],
                        fmax=mel_params["fmax"]
                    )

                    # OPRAVA: HiFi-GAN oƒçek√°v√° log-mel (v dB), ne power-mel
                    # Pou≈æijeme stabilnƒõj≈°√≠ logaritmickou transformaci
                    mel_log = np.log10(np.maximum(mel, 1e-5))

                    # 2. Resynt√©za pomoc√≠ HiFi-GAN (s blending pokud je intensity < 1.0)
                    # Pou≈æij per-request parametry (p≈ôedan√© z API)
                    refined_audio = self.vocoder.vocode(
                        mel_log,
                        sample_rate=sr,
                        original_audio=original_audio,
                        refinement_intensity=hifigan_refinement_intensity,
                        normalize_output=hifigan_normalize_output,
                        normalize_gain=hifigan_normalize_gain
                    )

                    if refined_audio is not None:
                        # Ulo≈æen√≠ vylep≈°en√©ho audio
                        sf.write(output_path, refined_audio, sr)
                        used_intensity = hifigan_refinement_intensity if hifigan_refinement_intensity is not None else config.HIFIGAN_REFINEMENT_INTENSITY
                        intensity_str = f" (intensity: {used_intensity:.2f})" if used_intensity is not None and used_intensity < 1.0 else ""
                        print(f"‚úÖ HiFi-GAN refinement dokonƒçen{intensity_str}")
                    else:
                        print("‚ö†Ô∏è HiFi-GAN vocoding vr√°til None, refinement p≈ôeskoƒçen")

                except Exception as e:
                    print(f"‚ö†Ô∏è Warning: HiFi-GAN refinement selhal: {e}")

            # Zmƒõna rychlosti pomoc√≠ time_stretch (pokud speed != 1.0)
            # POZN√ÅMKA: Mus√≠ b√Ωt a≈æ PO HiFi-GAN, aby se zmƒõna rychlosti nep≈ôepsala
            # XTTS m≈Ø≈æe nepodporovat parametr speed, tak≈æe pou≈æijeme post-processing
            speed_float = float(speed) if speed is not None else 1.0

            # Tolerance kv≈Øli float porovn√°n√≠
            if abs(speed_float - 1.0) > 0.001:
                # Preferujeme FFmpeg atempo: mƒõn√≠ tempo bez zmƒõny v√Ω≈°ky (pitch)
                try:
                    _progress(95, "speed", f"√öprava rychlosti na {speed_float}x‚Ä¶")
                    import os
                    import subprocess
                    from backend.audio_processor import AudioProcessor

                    if AudioProcessor._check_ffmpeg():
                        print(f"üéöÔ∏è  Aplikuji zmƒõnu rychlosti (tempo) p≈ôes FFmpeg atempo: {speed_float}x")
                        tmp_path = f"{output_path}.tmp_speed.wav"
                        # atempo podporuje 0.5‚Äì2.0 (co≈æ odpov√≠d√° validaci v API)
                        cmd = [
                            "ffmpeg",
                            "-hide_banner",
                            "-loglevel",
                            "error",
                            "-y",
                            "-i",
                            str(output_path),
                            "-filter:a",
                            f"atempo={speed_float}",
                            "-ar",
                            str(OUTPUT_SAMPLE_RATE),
                            tmp_path,
                        ]
                        subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)
                        os.replace(tmp_path, str(output_path))
                        print("‚úÖ Rychlost zmƒõnƒõna (FFmpeg atempo)")
                    else:
                        raise FileNotFoundError("FFmpeg nen√≠ dostupn√Ω")
                except Exception as e:
                    # Fallback bez FFmpeg: resample (zmƒõn√≠ i v√Ω≈°ku hlasu), ale rychlost bude fungovat
                    try:
                        import librosa
                        import soundfile as sf

                        print(
                            f"‚ö†Ô∏è  FFmpeg atempo nelze pou≈æ√≠t ({e}). "
                            f"Pou≈æiji fallback p≈ôes resampling (zmƒõn√≠ i v√Ω≈°ku): {speed_float}x"
                        )
                        audio, sr = librosa.load(output_path, sr=None)
                        # Pro rychlej≈°√≠ ≈ôeƒç pot≈ôebujeme m√©nƒõ sampl≈Ø => target_sr = sr / speed
                        target_sr = max(8000, int(sr / speed_float))
                        audio_rs = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)
                        # Zap√≠≈°eme p≈ôi p≈Øvodn√≠m sr -> efekt rychlosti (s posunem pitch)
                        sf.write(output_path, audio_rs, sr)
                        print("‚úÖ Rychlost zmƒõnƒõna (fallback resampling)")
                    except Exception as e2:
                        print(f"‚ö†Ô∏è Warning: Zmƒõna rychlosti selhala i ve fallbacku: {e2}, pokraƒçuji bez zmƒõny rychlosti")
            else:
                # Norm√°ln√≠ rychlost
                pass

            # Fin√°ln√≠ headroom (po V≈†EM): v≈ædy, aby UI headroom mƒõl efekt i kdy≈æ enhancement nebƒõ≈æ√≠ / sel≈æe,
            # a aby se headroom dorovnal po HiFi-GAN / zmƒõnƒõ rychlosti.
            try:
                _progress(97, "final", "Fin√°ln√≠ √∫pravy (headroom)‚Ä¶")
                import librosa
                import soundfile as sf

                audio, sr = librosa.load(output_path, sr=None)
                final_headroom_db = target_headroom_db if target_headroom_db is not None else OUTPUT_HEADROOM_DB
                if final_headroom_db is not None:
                    try:
                        # Headroom funguje jako "ceiling" (strop): pokud je peak nad c√≠lem, ztlum√≠me.
                        # Nechceme nikdy zesilovat tich√© v√Ωstupy, proto≈æe to p≈Øsob√≠, ≈æe posuvn√≠k "nefunguje".
                        peak = float(np.max(np.abs(audio))) if audio is not None and len(audio) else 0.0
                        if peak > 0:
                            if float(final_headroom_db) < 0:
                                target_peak = 10 ** (float(final_headroom_db) / 20.0)
                            else:
                                target_peak = 0.999

                            if peak > target_peak:
                                scale = target_peak / peak
                                audio = audio * scale
                                try:
                                    peak_after = float(np.max(np.abs(audio))) if audio is not None and len(audio) else 0.0
                                    print(
                                        f"üîâ Headroom ceiling detail: headroom_db={float(final_headroom_db):.1f} dB, "
                                        f"peak_before={peak:.4f}, target_peak={target_peak:.4f}, scale={scale:.4f}, peak_after={peak_after:.4f}"
                                    )
                                except Exception:
                                    pass
                            else:
                                # Pod c√≠lem nic nedƒõl√°me (nezesilujeme)
                                try:
                                    print(
                                        f"üîâ Headroom ceiling: headroom_db={float(final_headroom_db):.1f} dB, "
                                        f"peak_before={peak:.4f} <= target_peak={target_peak:.4f} (bez zmƒõny)"
                                    )
                                except Exception:
                                    pass

                        if not np.isfinite(audio).all():
                            audio = np.nan_to_num(audio, nan=0.0, posinf=0.0, neginf=0.0)
                    except Exception:
                        audio = np.clip(audio, -0.999, 0.999)

                    sf.write(output_path, audio, sr)
                    print(f"üîâ Fin√°ln√≠ headroom ceiling: {final_headroom_db} dB (aplikov√°no jen pokud peak p≈ôes√°hl c√≠l)")
            except Exception as e:
                print(f"‚ö†Ô∏è Warning: Fin√°ln√≠ headroom selhal: {e}")
            # 99% nech√°me a≈æ pro √∫plnƒõ posledn√≠ krok v backend/main.py (tƒõsnƒõ p≈ôed done=100),
            # a≈• to v UI nevypad√°, ≈æe je to "hotov√©", ale je≈°tƒõ dlouho to stoj√≠.
            _progress(96, "final", "Dokonƒçuji‚Ä¶")

        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            print(f"Generate error details:\n{error_details}")
            raise Exception(f"Chyba p≈ôi generov√°n√≠ ≈ôeƒçi: {str(e)}")



    async def warmup(self, demo_voice_path: Optional[str] = None):
        """
        Zah≈ôeje model prvn√≠m inference

        Args:
            demo_voice_path: Cesta k demo hlasu pro warmup
        """
        if not self.is_loaded:
            await self.load_model()

        if demo_voice_path and Path(demo_voice_path).exists():
            try:
                # Pou≈æij v√Ωchoz√≠ parametry pro warmup
                from backend.config import (
                    TTS_SPEED,
                    TTS_TEMPERATURE,
                    TTS_LENGTH_PENALTY,
                    TTS_REPETITION_PENALTY,
                    TTS_TOP_K,
                    TTS_TOP_P,
                    OUTPUTS_DIR
                )
                # Generuj warmup audio s kr√°tk√Ωm textem
                warmup_output = await self.generate(
                    text="Warmup.",
                    speaker_wav=demo_voice_path,
                    language="cs",
                    speed=TTS_SPEED,
                    temperature=TTS_TEMPERATURE,
                    length_penalty=TTS_LENGTH_PENALTY,
                    repetition_penalty=TTS_REPETITION_PENALTY,
                    top_k=TTS_TOP_K,
                    top_p=TTS_TOP_P
                )
                # Smazat warmup soubor, aby se neukl√°dal do historie
                warmup_path = Path(warmup_output)
                if warmup_path.exists():
                    try:
                        warmup_path.unlink()
                    except Exception:
                        pass  # Ignoruj chyby p≈ôi maz√°n√≠
                print("Model warmup dokonƒçen")
            except Exception as e:
                print(f"Warmup selhal: {str(e)}")

    async def generate_multi_pass(
        self,
        text: str,
        speaker_wav: str,
        language: str = "cs",
        speed: float = 1.0,
        temperature: float = 0.7,
        length_penalty: float = 1.0,
        repetition_penalty: float = 2.0,
        top_k: int = 50,
        top_p: float = 0.85,
        quality_mode: Optional[str] = None,
        enhancement_preset: Optional[str] = None,
        variant_count: int = 3,
        enable_batch: Optional[bool] = None,
        enable_vad: Optional[bool] = None,
        use_hifigan: bool = False,
        enable_normalization: bool = True,
        enable_denoiser: bool = True,
        enable_compressor: bool = True,
        enable_deesser: bool = True,
        enable_eq: bool = True,
        enable_trim: bool = True,
        enable_whisper: Optional[bool] = None,
        whisper_intensity: Optional[float] = None,
        target_headroom_db: Optional[float] = None,
        hifigan_refinement_intensity: Optional[float] = None,
        hifigan_normalize_output: Optional[bool] = None,
        hifigan_normalize_gain: Optional[float] = None,
        enable_enhancement: Optional[bool] = None,
        enable_dialect_conversion: Optional[bool] = None,
        dialect_code: Optional[str] = None,
        dialect_intensity: float = 1.0,
        job_id: Optional[str] = None
    ) -> List[dict]:
        """
        Generuje v√≠ce variant ≈ôeƒçi s r≈Øzn√Ωmi parametry

        Args:
            text: Text k synt√©ze
            speaker_wav: Cesta k audio souboru s hlasem
            language: Jazyk
            speed: Rychlost ≈ôeƒçi
            temperature: Z√°kladn√≠ teplota
            length_penalty: Length penalty
            repetition_penalty: Repetition penalty
            top_k: Top-k sampling
            top_p: Top-p sampling
            quality_mode: Quality preset
            enhancement_preset: Enhancement preset
            variant_count: Poƒçet variant k vygenerov√°n√≠
            enable_batch: Zapnout batch processing
            enable_vad: Zapnout VAD
            use_hifigan: Pou≈æ√≠t HiFi-GAN

        Returns:
            Seznam variant s metadaty
        """
        variants = []
        base_seed = 42

        # Variace teplot pro r≈Øzn√© varianty
        temperature_variations = [
            temperature - 0.1,
            temperature,
            temperature + 0.1
        ]

        for i in range(variant_count):
            if job_id:
                try:
                    from backend.progress_manager import ProgressManager
                    ProgressManager.update(
                        job_id,
                        percent=2 + (90.0 * i / max(1, variant_count)),
                        stage="multi_pass",
                        message=f"Generuji variantu {i+1}/{variant_count}‚Ä¶",
                        meta_update={"variant": i + 1, "variants_total": variant_count},
                    )
                except Exception:
                    pass
            variant_seed = base_seed + i
            variant_temp = temperature_variations[i % len(temperature_variations)]

            # Generuj variantu
            output_path = await self.generate(
                text=text,
                speaker_wav=speaker_wav,
                language=language,
                speed=speed,
                temperature=variant_temp,
                length_penalty=length_penalty,
                repetition_penalty=repetition_penalty,
                top_k=top_k,
                top_p=top_p,
                quality_mode=quality_mode,
                seed=variant_seed,
                enhancement_preset=enhancement_preset,
                enable_enhancement=enable_enhancement,
                multi_pass=False,  # Zabr√°n√≠ rekurzi
                enable_batch=enable_batch,
                enable_vad=enable_vad,
                use_hifigan=use_hifigan,
                enable_normalization=enable_normalization,
                enable_denoiser=enable_denoiser,
                enable_compressor=enable_compressor,
                enable_deesser=enable_deesser,
                enable_eq=enable_eq,
                enable_trim=enable_trim,
                enable_whisper=enable_whisper,
                whisper_intensity=whisper_intensity,
                target_headroom_db=target_headroom_db,
                hifigan_refinement_intensity=hifigan_refinement_intensity,
                hifigan_normalize_output=hifigan_normalize_output,
                hifigan_normalize_gain=hifigan_normalize_gain,
                enable_dialect_conversion=enable_dialect_conversion,
                dialect_code=dialect_code,
                dialect_intensity=dialect_intensity,
                job_id=job_id
            )

            filename = Path(output_path).name
            audio_url = f"/api/audio/{filename}"

            variants.append({
                "audio_url": audio_url,
                "filename": filename,
                "seed": variant_seed,
                "temperature": variant_temp,
                "index": i + 1
            })

        return variants

    async def generate_batch(
        self,
        text: str,
        speaker_wav: str,
        language: str = "cs",
        speed: float = 1.0,
        temperature: float = 0.7,
        length_penalty: float = 1.0,
        repetition_penalty: float = 2.0,
        top_k: int = 50,
        top_p: float = 0.85,
        quality_mode: Optional[str] = None,
        seed: Optional[int] = None,
        enhancement_preset: Optional[str] = None,
        enable_vad: Optional[bool] = None,
        use_hifigan: bool = False,
        enable_normalization: bool = True,
        enable_denoiser: bool = True,
        enable_compressor: bool = True,
        enable_deesser: bool = True,
        enable_eq: bool = True,
        enable_trim: bool = True,
        enable_dialect_conversion: Optional[bool] = None,
        dialect_code: Optional[str] = None,
        dialect_intensity: float = 1.0,
        job_id: Optional[str] = None
    ) -> str:
        """
        Generuje ≈ôeƒç pro dlouh√Ω text pomoc√≠ batch processing

        Args:
            text: Text k synt√©ze
            speaker_wav: Cesta k audio souboru s hlasem
            language: Jazyk
            speed: Rychlost ≈ôeƒçi
            temperature: Teplota
            length_penalty: Length penalty
            repetition_penalty: Repetition penalty
            top_k: Top-k sampling
            top_p: Top-p sampling
            quality_mode: Quality preset
            seed: Seed
            enhancement_preset: Enhancement preset
            enable_vad: Zapnout VAD
            use_hifigan: Pou≈æ√≠t HiFi-GAN

        Returns:
            Cesta k fin√°ln√≠mu spojen√©mu audio souboru
        """
        from backend.audio_concatenator import AudioConcatenator

        # Rozdƒõl text na ƒç√°sti podle XTTS token≈Ø (ochrana proti limitu 400 token≈Ø)
        chunks = self._split_text_by_xtts_tokens(text, language=language)
        token_counts = [self._count_xtts_tokens(c, language) for c in chunks]
        # fallback na d√©lku v znac√≠ch, pokud tokenizer nen√≠ k dispozici
        units = [(tc if tc is not None and tc > 0 else max(1, len(ch))) for tc, ch in zip(token_counts, chunks)]
        total_units = max(1, sum(units))
        done_units = 0

        if job_id:
            try:
                from backend.progress_manager import ProgressManager
                ProgressManager.update(
                    job_id,
                    percent=3,
                    stage="batch_prepare",
                    message=f"Rozdƒõleno na {len(chunks)} ƒç√°st√≠‚Ä¶",
                    meta_update={"chunks_total": len(chunks), "total_units": total_units, "unit": "tokens_or_chars"},
                )
            except Exception:
                pass

        if len(chunks) == 1:
            # Pokud je jen jedna ƒç√°st, pou≈æij standardn√≠ generov√°n√≠
            return await self.generate(
                text=text,
                speaker_wav=speaker_wav,
                language=language,
                speed=speed,
                temperature=temperature,
                length_penalty=length_penalty,
                repetition_penalty=repetition_penalty,
                top_k=top_k,
                top_p=top_p,
                quality_mode=quality_mode,
                seed=seed,
                enhancement_preset=enhancement_preset,
                multi_pass=False,
                enable_batch=False,
                enable_vad=enable_vad,
                use_hifigan=use_hifigan,
                enable_normalization=enable_normalization,
                enable_denoiser=enable_denoiser,
                enable_compressor=enable_compressor,
                enable_deesser=enable_deesser,
                enable_eq=enable_eq,
                enable_trim=enable_trim,
                enable_dialect_conversion=enable_dialect_conversion,
                dialect_code=dialect_code,
                dialect_intensity=dialect_intensity,
                job_id=job_id
            )

        print(f"üì¶ Batch processing: rozdƒõleno na {len(chunks)} ƒç√°st√≠")

        # Generuj ka≈ædou ƒç√°st
        audio_files = []
        for i, chunk in enumerate(chunks):
            if job_id:
                try:
                    from backend.progress_manager import ProgressManager
                    # ETA: odhad z u≈æ hotov√Ωch ƒç√°st√≠ (sekundy / unit), po 1. ƒç√°sti je to u≈æ celkem stabiln√≠
                    now = time.time()
                    started_at = ProgressManager.get(job_id).get("started_at", now)  # type: ignore[union-attr]
                    elapsed = max(0.0, now - float(started_at))
                    rate = (elapsed / done_units) if done_units > 0 else None
                    remaining = max(0, total_units - done_units)
                    eta = int(rate * remaining) if rate is not None else None

                    percent = 5 + (85.0 * done_units / total_units)
                    ProgressManager.update(
                        job_id,
                        percent=percent,
                        eta_seconds=eta,
                        stage="batch",
                        message=f"Generuji ƒç√°st {i+1}/{len(chunks)}‚Ä¶",
                        meta_update={"chunk": i + 1, "chunks_total": len(chunks), "done_units": done_units},
                    )
                except Exception:
                    pass
            print(f"   Generuji ƒç√°st {i+1}/{len(chunks)}...")
            chunk_output = await self.generate(
                text=chunk,
                speaker_wav=speaker_wav,
                language=language,
                speed=speed,
                temperature=temperature,
                length_penalty=length_penalty,
                repetition_penalty=repetition_penalty,
                top_k=top_k,
                top_p=top_p,
                quality_mode=quality_mode,
                seed=seed,
                enhancement_preset=enhancement_preset,
                multi_pass=False,
                enable_batch=False,
                enable_vad=enable_vad,
                use_hifigan=use_hifigan,
                enable_normalization=enable_normalization,
                enable_denoiser=enable_denoiser,
                enable_compressor=enable_compressor,
                enable_deesser=enable_deesser,
                enable_eq=enable_eq,
                enable_trim=enable_trim,
                enable_dialect_conversion=enable_dialect_conversion,
                dialect_code=dialect_code,
                dialect_intensity=dialect_intensity,
                job_id=job_id
            )
            audio_files.append(chunk_output)
            done_units += units[i]

            if job_id:
                try:
                    from backend.progress_manager import ProgressManager
                    now = time.time()
                    started_at = ProgressManager.get(job_id).get("started_at", now)  # type: ignore[union-attr]
                    elapsed = max(0.0, now - float(started_at))
                    rate = elapsed / max(1, done_units)
                    remaining = max(0, total_units - done_units)
                    eta = int(rate * remaining)
                    percent = 5 + (85.0 * done_units / total_units)
                    ProgressManager.update(
                        job_id,
                        percent=percent,
                        eta_seconds=eta,
                        stage="batch",
                        message=f"Hotovo {i+1}/{len(chunks)} ƒç√°st√≠‚Ä¶",
                        meta_update={"done_units": done_units},
                    )
                except Exception:
                    pass

        # Spoj audio ƒç√°sti
        output_filename = f"{uuid.uuid4()}.wav"
        output_path = OUTPUTS_DIR / output_filename

        print(f"üîó Spojuji {len(audio_files)} audio ƒç√°st√≠...")
        if job_id:
            try:
                from backend.progress_manager import ProgressManager
                # concat + post tvo≈ô√≠ posledn√≠ch ~10‚Äì15%
                ProgressManager.update(job_id, percent=92, stage="concat", message="Spojuji ƒç√°sti‚Ä¶", eta_seconds=5)
            except Exception:
                pass
        AudioConcatenator.concatenate_audio(
            audio_files,
            str(output_path),
            crossfade_ms=50
        )

        # Smazat doƒçasn√© ƒç√°sti
        for audio_file in audio_files:
            try:
                Path(audio_file).unlink()
            except:
                pass

        print(f"‚úÖ Batch processing dokonƒçen: {output_path}")
        if job_id:
            try:
                from backend.progress_manager import ProgressManager
                ProgressManager.update(job_id, percent=95, stage="post", message="Dokonƒçuji‚Ä¶")
            except Exception:
                pass
        return str(output_path)

    async def generate_multi_lang_speaker(
        self,
        text: str,
        default_speaker_wav: str,
        default_language: str = "cs",
        speaker_map: Optional[Dict[str, str]] = None,
        job_id: Optional[str] = None,
        **kwargs
    ) -> str:
        """
        Generuje ≈ôeƒç pro text s v√≠ce jazyky a mluvƒç√≠mi

        Podporuje syntaxi: [lang:speaker]text[/lang] nebo [lang]text[/lang]

        Args:
            text: Text s anotacemi [lang:speaker]text[/lang]
            default_speaker_wav: V√Ωchoz√≠ mluvƒç√≠ pro neanotovan√© ƒç√°sti
            default_language: V√Ωchoz√≠ jazyk
            speaker_map: Mapov√°n√≠ speaker_id -> speaker_wav_path
            job_id: ID jobu pro progress tracking
            **kwargs: Ostatn√≠ parametry (speed, temperature, atd.)

        Returns:
            Cesta k fin√°ln√≠mu audio souboru
        """
        from backend.multi_lang_speaker_processor import MultiLangSpeakerProcessor
        import re

        # Nejd≈ô√≠ve zpracuj pauzy - rozsekej text podle [pause:ms] a pak parsuj ka≈æd√Ω kus
        # Podporovan√© formy: [pause], [pause:200], [pause=200], [pause:200ms]
        pause_re = re.compile(r"\[pause(?:\s*[:=]\s*(\d+)\s*(?:ms)?)?\]", re.IGNORECASE)
        pause_matches = list(pause_re.finditer(text))

        # Pokud jsou v textu pauzy, rozsekej text a zpracuj ka≈æd√Ω kus zvl√°≈°≈•
        if pause_matches:
            print(f"‚è∏Ô∏è  Detekov√°ny pauzy v multi-lang textu: {len(pause_matches)} pauz")
            text_parts = []
            pauses_between = []
            last_pos = 0

            for m in pause_matches:
                # Text p≈ôed pauzou
                part_before = text[last_pos:m.start()].strip()
                if part_before:
                    text_parts.append(part_before)

                # D√©lka pauzy
                dur_raw = m.group(1)
                try:
                    dur = int(dur_raw) if dur_raw is not None else 500
                except Exception:
                    dur = 500
                dur = max(0, min(dur, 10000))  # 0‚Äì10s safety
                pauses_between.append(dur)

                last_pos = m.end()

            # Zbytek textu po posledn√≠ pauze
            tail = text[last_pos:].strip()
            if tail:
                text_parts.append(tail)

            # Pokud m√°me ƒç√°sti s pauzami, zpracuj ka≈ædou ƒç√°st zvl√°≈°≈• a spoj s pauzami
            if len(text_parts) > 1:
                print(f"   Rozdƒõleno na {len(text_parts)} ƒç√°st√≠ s {len(pauses_between)} pauzami")
                audio_files = []

                # Vytvo≈ô processor
                default_lang = default_language if default_language else "cs"
                processor = MultiLangSpeakerProcessor(
                    default_language=default_lang,
                    default_speaker=default_speaker_wav
                )

                # Registruj mluvƒç√≠
                if speaker_map:
                    for speaker_id, speaker_wav in speaker_map.items():
                        processor.register_speaker(speaker_id, speaker_wav)

                # Zpracuj ka≈ædou ƒç√°st zvl√°≈°≈•
                for i, part_text in enumerate(text_parts):
                    part_segments = processor.parse_text(part_text)

                    # Pokud m√° ƒç√°st jen jeden segment, pou≈æij standardn√≠ generov√°n√≠
                    if len(part_segments) == 1:
                        seg = part_segments[0]
                        part_audio = await self.generate(
                            text=seg.text,
                            speaker_wav=seg.speaker_wav or default_speaker_wav,
                            language=seg.language,
                            enable_batch=False,
                            handle_pauses=False,  # Pauzy u≈æ jsme zpracovali
                            job_id=None,
                            **kwargs
                        )
                        audio_files.append(part_audio)
                    else:
                        # V√≠ce segment≈Ø v ƒç√°sti - generuj ka≈æd√Ω segment zvl√°≈°≈• a spoj
                        part_audio_files = []
                        for seg in part_segments:
                            # Odstra≈à enable_trim z kwargs, proto≈æe ho explicitnƒõ nastavujeme
                            seg_kwargs = {k: v for k, v in kwargs.items() if k != 'enable_trim'}
                            seg_audio = await self.generate(
                                text=seg.text,
                                speaker_wav=seg.speaker_wav or default_speaker_wav,
                                language=seg.language,
                                enable_batch=False,
                                handle_pauses=False,
                                enable_trim=False,
                                job_id=None,
                                **seg_kwargs
                            )
                            part_audio_files.append(seg_audio)

                        # Spoj segmenty ƒç√°sti
                        from backend.audio_concatenator import AudioConcatenator
                        temp_output = OUTPUTS_DIR / f"{uuid.uuid4()}.wav"
                        AudioConcatenator.concatenate_audio(
                            part_audio_files,
                            str(temp_output),
                            crossfade_ms=100
                        )
                        # Uklidit doƒçasn√© segmenty
                        for af in part_audio_files:
                            try:
                                Path(af).unlink()
                            except Exception:
                                pass
                        part_audio = str(temp_output)
                        audio_files.append(part_audio)

                    # P≈ôidej pauzu po ƒç√°sti (kromƒõ posledn√≠)
                    if i < len(pauses_between):
                        pause_ms = pauses_between[i]
                        # Pauza se p≈ôid√° p≈ôi spojov√°n√≠

                # Spoj v≈°echny ƒç√°sti s pauzami
                from backend.audio_concatenator import AudioConcatenator
                output_filename = f"{uuid.uuid4()}.wav"
                output_path = OUTPUTS_DIR / output_filename

                # Spoj s pauzami
                concatenated_audio = []
                import librosa
                import soundfile as sf
                import numpy as np
                sr = OUTPUT_SAMPLE_RATE

                for i, audio_file in enumerate(audio_files):
                    audio, _ = librosa.load(audio_file, sr=sr)
                    concatenated_audio.append(audio)

                    # P≈ôidej pauzu po ƒç√°sti (kromƒõ posledn√≠)
                    if i < len(pauses_between):
                        pause_ms = pauses_between[i]
                        pause_samples = int(pause_ms * sr / 1000)
                        if pause_samples > 0:
                            print(f"‚è±Ô∏è  Pause[{i}]: {pause_ms} ms => {pause_samples} samples")
                            concatenated_audio.append(np.zeros(pause_samples, dtype=np.float32))

                final_audio = np.concatenate(concatenated_audio) if concatenated_audio else np.array([], dtype=np.float32)
                sf.write(str(output_path), final_audio, sr)

                # Uklidit doƒçasn√© soubory
                for audio_file in audio_files:
                    try:
                        Path(audio_file).unlink()
                    except Exception:
                        pass

                print(f"‚úÖ Multi-lang/speaker generov√°n√≠ s pauzami dokonƒçeno: {output_path}")
                return str(output_path)

        # Pokud nejsou pauzy, pokraƒçuj norm√°lnƒõ
        # Vytvo≈ô processor
        # V√Ωchoz√≠ jazyk je ƒçe≈°tina, pokud nen√≠ zad√°n
        default_lang = default_language if default_language else "cs"
        processor = MultiLangSpeakerProcessor(
            default_language=default_lang,
            default_speaker=default_speaker_wav
        )

        # Registruj mluvƒç√≠
        if speaker_map:
            for speaker_id, speaker_wav in speaker_map.items():
                processor.register_speaker(speaker_id, speaker_wav)

        # Parsuj text na segmenty
        segments = processor.parse_text(text)

        if job_id:
            try:
                from backend.progress_manager import ProgressManager
                ProgressManager.update(
                    job_id,
                    percent=2,
                    stage="parse",
                    message=f"Parsov√°no {len(segments)} segment≈Ø‚Ä¶",
                    meta_update={"segments_total": len(segments)}
                )
            except Exception:
                pass

        print(f"üìù Multi-lang/speaker: parsov√°no {len(segments)} segment≈Ø")
        if len(segments) > 1:
            print(processor.get_segments_summary(segments))

        if len(segments) == 1:
            # Jen jeden segment - pou≈æij standardn√≠ generov√°n√≠
            segment = segments[0]

            # Pro cross-language generov√°n√≠ uprav parametry
            segment_kwargs = kwargs.copy()
            speaker_wav_path = segment.speaker_wav or default_speaker_wav
            is_cross_language = False

            # Detekce cross-language: pokud je jazyk jin√Ω ne≈æ cs a hlas je pravdƒõpodobnƒõ ƒçesk√Ω
            if segment.language != "cs" and speaker_wav_path:
                speaker_name = Path(speaker_wav_path).stem.lower()
                czech_indicators = ['buchty', 'klepl', 'bohumil', 'werich', 'pohadka', 'brodsky', 'speakato']
                if any(indicator in speaker_name for indicator in czech_indicators):
                    is_cross_language = True
                    print(f"‚ö†Ô∏è Cross-language detekce: pou≈æ√≠v√° se ƒçesk√Ω hlas ({speaker_name}) pro jazyk {segment.language}")
                    print(f"   Pro lep≈°√≠ kvalitu doporuƒçujeme pou≈æ√≠t hlas v jazyce {segment.language}")
                    # Uprav parametry pro cross-language
                    if 'temperature' not in segment_kwargs or segment_kwargs.get('temperature', 0.7) < 0.5:
                        segment_kwargs['temperature'] = 0.7
                    if 'length_penalty' not in segment_kwargs or segment_kwargs.get('length_penalty', 1.0) > 1.2:
                        segment_kwargs['length_penalty'] = 1.0
                    if 'repetition_penalty' not in segment_kwargs or segment_kwargs.get('repetition_penalty', 2.0) < 1.5:
                        segment_kwargs['repetition_penalty'] = 2.0
                    print(f"   Upraven√© parametry pro cross-language: temp={segment_kwargs.get('temperature', 0.7)}, length_penalty={segment_kwargs.get('length_penalty', 1.0)}")

            result = await self.generate(
                text=segment.text,
                speaker_wav=speaker_wav_path,
                language=segment.language,
                enable_batch=False,  # Batch u≈æ ≈ôe≈°√≠me na √∫rovni segment≈Ø
                job_id=job_id,
                **segment_kwargs
            )

            # Pro kr√°tk√© texty (1-3 slova) pou≈æij agresivnƒõj≈°√≠ trimov√°n√≠
            # XTTS ƒçasto generuje dlouh√© ticho pro velmi kr√°tk√© texty
            # POZN√ÅMKA: Trimov√°n√≠ se prov√°d√≠ v _generate_sync P≈òED upsamplingem,
            # tak≈æe tady jen kontrolujeme d√©lku a p≈ô√≠padnƒõ omez√≠me
            word_count = len(segment.text.split())
            if word_count <= 3:
                try:
                    import librosa
                    import soundfile as sf
                    audio, sr = librosa.load(result, sr=None)
                    original_length = len(audio) / sr

                    # Maxim√°ln√≠ d√©lka pro kr√°tk√© texty (5 sekund)
                    max_duration_samples = int(5.0 * sr)
                    if len(audio) > max_duration_samples:
                        print(f"‚ö†Ô∏è Kr√°tk√Ω segment ({word_count} slova) je p≈ô√≠li≈° dlouh√Ω ({len(audio)/sr:.1f}s), o≈ôez√°v√°m na 5s")
                        audio = audio[:max_duration_samples]
                        sf.write(result, audio, sr)
                        print(f"‚úÇÔ∏è Fin√°ln√≠ o≈ôez kr√°tk√©ho segmentu: {original_length:.1f}s ‚Üí {len(audio)/sr:.1f}s")
                except Exception as e:
                    print(f"‚ö†Ô∏è Warning: Fin√°ln√≠ o≈ôez kr√°tk√©ho segmentu selhal: {e}")

            return result

        # Generuj ka≈æd√Ω segment zvl√°≈°≈•
        audio_files = []
        for i, segment in enumerate(segments):
            if job_id:
                try:
                    from backend.progress_manager import ProgressManager
                    ProgressManager.update(
                        job_id,
                        percent=5 + (85.0 * i / max(1, len(segments))),
                        stage="multi_segment",
                        message=f"Generuji segment {i+1}/{len(segments)} ({segment.language})‚Ä¶",
                        meta_update={"segment": i + 1, "segments_total": len(segments), "language": segment.language}
                    )
                except Exception:
                    pass

            print(f"üé§ Generuji segment {i+1}/{len(segments)}: lang={segment.language}, speaker={segment.speaker_id or 'default'}")

            # Odstra≈à enable_trim z kwargs, proto≈æe ho explicitnƒõ nastavujeme
            segment_kwargs = {k: v for k, v in kwargs.items() if k != 'enable_trim'}

            # Pro cross-language generov√°n√≠ (nap≈ô. ƒçesk√Ω hlas pro anglick√Ω text) pou≈æij lep≈°√≠ parametry
            # XTTS m≈Ø≈æe m√≠t probl√©my s cross-language cloning, tak≈æe uprav√≠me parametry
            speaker_wav_path = segment.speaker_wav or default_speaker_wav
            is_cross_language = False

            # Detekce cross-language: pokud je jazyk jin√Ω ne≈æ cs a hlas je pravdƒõpodobnƒõ ƒçesk√Ω
            if segment.language != "cs" and speaker_wav_path:
                # Zkontroluj n√°zev souboru - pokud obsahuje ƒçesk√© n√°zvy, je to cross-language
                speaker_name = Path(speaker_wav_path).stem.lower()
                czech_indicators = ['buchty', 'klepl', 'bohumil', 'werich', 'pohadka', 'brodsky', 'speakato']
                if any(indicator in speaker_name for indicator in czech_indicators):
                    is_cross_language = True
                    print(f"‚ö†Ô∏è Cross-language detekce: pou≈æ√≠v√° se ƒçesk√Ω hlas ({speaker_name}) pro jazyk {segment.language}")
                    print(f"   Pro lep≈°√≠ kvalitu doporuƒçujeme pou≈æ√≠t hlas v jazyce {segment.language}")
                    # Uprav parametry pro cross-language - vy≈°≈°√≠ temperature, ni≈æ≈°√≠ length_penalty
                    if 'temperature' not in segment_kwargs or segment_kwargs.get('temperature', 0.7) < 0.5:
                        segment_kwargs['temperature'] = 0.7  # Vy≈°≈°√≠ temperature pro lep≈°√≠ cross-language
                    if 'length_penalty' not in segment_kwargs or segment_kwargs.get('length_penalty', 1.0) > 1.2:
                        segment_kwargs['length_penalty'] = 1.0  # Ni≈æ≈°√≠ length_penalty pro krat≈°√≠ generov√°n√≠
                    if 'repetition_penalty' not in segment_kwargs or segment_kwargs.get('repetition_penalty', 2.0) < 1.5:
                        segment_kwargs['repetition_penalty'] = 2.0  # Vy≈°≈°√≠ repetition_penalty pro lep≈°√≠ kvalitu
                    print(f"   Upraven√© parametry pro cross-language: temp={segment_kwargs.get('temperature', 0.7)}, length_penalty={segment_kwargs.get('length_penalty', 1.0)}")

            segment_audio = await self.generate(
                text=segment.text,
                speaker_wav=speaker_wav_path,
                language=segment.language,
                enable_batch=False,  # Batch u≈æ ≈ôe≈°√≠me na √∫rovni segment≈Ø
                handle_pauses=False,  # Pauzy ≈ôe≈°√≠me na √∫rovni spojov√°n√≠
                enable_trim=False,  # Vypneme trim pro jednotliv√© segmenty - trimneme a≈æ p≈ôi spojov√°n√≠
                job_id=None,  # Nep≈ôed√°v√°me job_id do jednotliv√Ωch segment≈Ø
                **segment_kwargs
            )

            # Pro kr√°tk√© texty (1-3 slova) pou≈æij kontrolu d√©lky p≈ôed spojen√≠m
            # POZN√ÅMKA: Trimov√°n√≠ se prov√°d√≠ v _generate_sync P≈òED upsamplingem,
            # tak≈æe tady jen kontrolujeme d√©lku a p≈ô√≠padnƒõ omez√≠me
            word_count = len(segment.text.split())
            if word_count <= 3:
                try:
                    import librosa
                    import soundfile as sf
                    audio, sr = librosa.load(segment_audio, sr=None)
                    original_length = len(audio) / sr

                    # Maxim√°ln√≠ d√©lka pro kr√°tk√© texty (5 sekund)
                    max_duration_samples = int(5.0 * sr)
                    if len(audio) > max_duration_samples:
                        print(f"‚ö†Ô∏è Kr√°tk√Ω segment {i+1} ({word_count} slova) je p≈ô√≠li≈° dlouh√Ω ({len(audio)/sr:.1f}s), o≈ôez√°v√°m na 5s")
                        audio = audio[:max_duration_samples]
                        sf.write(segment_audio, audio, sr)
                        print(f"‚úÇÔ∏è Fin√°ln√≠ o≈ôez segmentu {i+1}: {original_length:.1f}s ‚Üí {len(audio)/sr:.1f}s")
                except Exception as e:
                    print(f"‚ö†Ô∏è Warning: Fin√°ln√≠ o≈ôez kr√°tk√©ho segmentu selhal: {e}")

            audio_files.append(segment_audio)

        # Spoj v≈°echny segmenty
        from backend.audio_concatenator import AudioConcatenator

        output_filename = f"{uuid.uuid4()}.wav"
        output_path = OUTPUTS_DIR / output_filename

        if job_id:
            try:
                from backend.progress_manager import ProgressManager
                ProgressManager.update(job_id, percent=92, stage="concat", message="Spojuji segmenty‚Ä¶")
            except Exception:
                pass

        print(f"üîó Spojuji {len(audio_files)} audio segment≈Ø...")
        AudioConcatenator.concatenate_audio(
            audio_files,
            str(output_path),
            crossfade_ms=100  # Zv√Ω≈°en√Ω crossfade pro plynulej≈°√≠ p≈ôechody (100ms m√≠sto 50ms)
        )

        # Uklidit doƒçasn√© soubory
        for audio_file in audio_files:
            try:
                Path(audio_file).unlink()
            except Exception:
                pass

        print(f"‚úÖ Multi-lang/speaker generov√°n√≠ dokonƒçeno: {output_path}")
        return str(output_path)

    def get_status(self) -> dict:
        """Vr√°t√≠ status modelu"""
        return {
            "loaded": self.is_loaded,
            "loading": self.is_loading,
            "device": self.device,
            "cuda_available": torch.cuda.is_available(),
            "device_forced": DEVICE_FORCED,
            "force_device": FORCE_DEVICE,
            "gpu_name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
            "hifigan_available": self.vocoder.available if hasattr(self, 'vocoder') else False
        }

