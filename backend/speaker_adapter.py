"""
Speaker Adaptation modul pro caching a optimalizaci speaker embeddingÅ¯
"""
import hashlib
import pickle
import numpy as np
from pathlib import Path
from typing import Optional, Tuple
from backend.config import ENABLE_SPEAKER_CACHE, SPEAKER_CACHE_DIR
import torch


class SpeakerAdapter:
    """TÅ™Ã­da pro caching a optimalizaci speaker embeddingÅ¯"""

    def __init__(self):
        self.cache_dir = SPEAKER_CACHE_DIR
        self.cache_dir.mkdir(exist_ok=True)
        self.enabled = ENABLE_SPEAKER_CACHE

    def _get_cache_key(self, speaker_wav_path: str) -> str:
        """
        VytvoÅ™Ã­ cache klÃ­Ä z cesty k audio souboru

        Args:
            speaker_wav_path: Cesta k speaker audio souboru

        Returns:
            Cache klÃ­Ä (hash)
        """
        # PouÅ¾ij hash cesty a velikosti souboru pro jedineÄnost
        path_str = str(Path(speaker_wav_path).resolve())
        try:
            file_size = Path(speaker_wav_path).stat().st_size
            key_data = f"{path_str}:{file_size}"
        except:
            key_data = path_str

        return hashlib.md5(key_data.encode()).hexdigest()

    def _get_cache_path(self, cache_key: str) -> Path:
        """
        VrÃ¡tÃ­ cestu k cache souboru

        Args:
            cache_key: Cache klÃ­Ä

        Returns:
            Cesta k cache souboru
        """
        return self.cache_dir / f"{cache_key}.pkl"

    def get_speaker_embedding(
        self,
        speaker_wav_path: str,
        tts_model
    ) -> Optional[torch.Tensor]:
        """
        ZÃ­skÃ¡ speaker embedding z cache nebo extrahuje z audio

        Args:
            speaker_wav_path: Cesta k speaker audio souboru
            tts_model: TTS model instance

        Returns:
            Speaker embedding tensor nebo None
        """
        if not self.enabled:
            return None

        cache_key = self._get_cache_key(speaker_wav_path)
        cache_path = self._get_cache_path(cache_key)

        # Zkus naÄÃ­st z cache
        if cache_path.exists():
            try:
                with open(cache_path, 'rb') as f:
                    cached_data = pickle.load(f)
                    print(f"âœ… Speaker embedding naÄten z cache: {cache_key[:8]}...")
                    return cached_data
            except Exception as e:
                print(f"Warning: Failed to load speaker cache: {e}")

        # Pokud nenÃ­ v cache, extrahuj z modelu
        try:
            embedding = self._extract_embedding(speaker_wav_path, tts_model)
            if embedding is not None:
                # UloÅ¾ do cache
                try:
                    with open(cache_path, 'wb') as f:
                        # UklÃ¡dej na CPU (bez vazby na konkrÃ©tnÃ­ device)
                        pickle.dump(embedding.detach().cpu(), f)
                    print(f"ğŸ’¾ Speaker embedding uloÅ¾en do cache: {cache_key[:8]}...")
                except Exception as e:
                    print(f"Warning: Failed to save speaker cache: {e}")

            return embedding
        except Exception as e:
            print(f"Warning: Failed to extract speaker embedding: {e}")
            return None

    def get_conditioning_latents(
        self,
        speaker_wav_path: str,
        tts_model
    ) -> Optional[Tuple[torch.Tensor, torch.Tensor]]:
        """
        VrÃ¡tÃ­ (gpt_cond_latent, speaker_embedding) z cache nebo je spoÄÃ­tÃ¡.
        Pokud verze TTS neumoÅ¾Åˆuje extrakci, vrÃ¡tÃ­ None.
        """
        if not self.enabled:
            return None

        cache_key = self._get_cache_key(speaker_wav_path)
        cache_path = self._get_cache_path(f"cond_{cache_key}")

        if cache_path.exists():
            try:
                with open(cache_path, "rb") as f:
                    data = pickle.load(f)
                gpt = data.get("gpt_cond_latent")
                emb = data.get("speaker_embedding")
                if gpt is not None and emb is not None:
                    print(f"âœ… Conditioning latents naÄteny z cache: {cache_key[:8]}...")
                    return gpt, emb
            except Exception as e:
                print(f"Warning: Failed to load conditioning cache: {e}")

        try:
            gpt, emb = self._extract_conditioning_latents(speaker_wav_path, tts_model)
            if gpt is None or emb is None:
                return None
            try:
                with open(cache_path, "wb") as f:
                    pickle.dump(
                        {
                            "gpt_cond_latent": gpt.detach().cpu(),
                            "speaker_embedding": emb.detach().cpu(),
                        },
                        f,
                    )
                print(f"ğŸ’¾ Conditioning latents uloÅ¾eny do cache: {cache_key[:8]}...")
            except Exception as e:
                print(f"Warning: Failed to save conditioning cache: {e}")
            return gpt, emb
        except Exception as e:
            print(f"Warning: Failed to extract conditioning latents: {e}")
            return None

    def _extract_embedding(
        self,
        speaker_wav_path: str,
        tts_model
    ) -> Optional[torch.Tensor]:
        """
        Extrahuje speaker embedding z audio pomocÃ­ TTS modelu

        Args:
            speaker_wav_path: Cesta k speaker audio souboru
            tts_model: TTS model instance

        Returns:
            Speaker embedding tensor nebo None
        """
        try:
            # XTTS model mÃ¡ metodu pro extrakci speaker embeddingu
            # Zkus rÅ¯znÃ© moÅ¾nÃ© metody podle verze TTS
            if hasattr(tts_model, 'synthesizer'):
                synthesizer = tts_model.synthesizer
                if hasattr(synthesizer, 'get_conditioning_latents'):
                    # XTTS-v2 metoda
                    gpt_cond_latent, speaker_embedding, _ = synthesizer.get_conditioning_latents(
                        audio_path=speaker_wav_path
                    )
                    return speaker_embedding
                elif hasattr(synthesizer, 'compute_speaker_embedding'):
                    # AlternativnÃ­ metoda
                    return synthesizer.compute_speaker_embedding(speaker_wav_path)
            elif hasattr(tts_model, 'get_speaker_embedding'):
                return tts_model.get_speaker_embedding(speaker_wav_path)

            # Pokud Å¾Ã¡dnÃ¡ metoda nefunguje, vraÅ¥ None
            print("Warning: Speaker embedding extraction not available in this TTS version")
            return None

        except Exception as e:
            print(f"Error extracting speaker embedding: {e}")
            return None

    def _extract_conditioning_latents(
        self,
        speaker_wav_path: str,
        tts_model
    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor]]:
        """
        PokusÃ­ se vytÃ¡hnout conditioning latents ze synthesizeru (XTTS-v2).
        """
        try:
            if hasattr(tts_model, "synthesizer"):
                synthesizer = tts_model.synthesizer
                if hasattr(synthesizer, "get_conditioning_latents"):
                    gpt_cond_latent, speaker_embedding, _ = synthesizer.get_conditioning_latents(
                        audio_path=speaker_wav_path
                    )
                    return gpt_cond_latent, speaker_embedding
            return None, None
        except Exception as e:
            print(f"Error extracting conditioning latents: {e}")
            return None, None

    def clear_cache(self, speaker_wav_path: Optional[str] = None) -> int:
        """
        VymaÅ¾e cache pro konkrÃ©tnÃ­ speaker nebo celou cache

        Args:
            speaker_wav_path: Cesta k speaker audio (None = vymaÅ¾e vÅ¡e)

        Returns:
            PoÄet smazanÃ½ch souborÅ¯
        """
        if speaker_wav_path:
            cache_key = self._get_cache_key(speaker_wav_path)
            cache_path = self._get_cache_path(cache_key)
            if cache_path.exists():
                cache_path.unlink()
                return 1
            return 0
        else:
            # VymaÅ¾ celou cache
            count = 0
            for cache_file in self.cache_dir.glob("*.pkl"):
                cache_file.unlink()
                count += 1
            return count

    def get_cache_size(self) -> int:
        """
        VrÃ¡tÃ­ poÄet poloÅ¾ek v cache

        Returns:
            PoÄet cache souborÅ¯
        """
        return len(list(self.cache_dir.glob("*.pkl")))


# GlobÃ¡lnÃ­ instance
_speaker_adapter = None


def get_speaker_adapter() -> SpeakerAdapter:
    """VrÃ¡tÃ­ globÃ¡lnÃ­ instanci speaker adapteru"""
    global _speaker_adapter
    if _speaker_adapter is None:
        _speaker_adapter = SpeakerAdapter()
    return _speaker_adapter




















